{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a search engine for emojis\n",
    "\n",
    "1. Index the corpus\n",
    "\n",
    "term - token\n",
    "\n",
    "term - emoji index. A sparse matrix with true/false if emoji appears with term\n",
    "inverted index - dictionary of terms, and a list of their appearances (emojis)\n",
    "\n",
    "Building index:\n",
    "1. collect documents (sentences with emojis)\n",
    "2. tokenize the documents\n",
    "3. preprocess the tokens. lowercase, cleanup, english\n",
    "4. Index documents with inverted index\n",
    "\n",
    "Each emoji has unique ID\n",
    "Maintain dictionary and postings\n",
    "dictionary - emoji and pointer to document its from\n",
    "postings - inverted index [emoji, frequency in doc, [docID1, docID2]]\n",
    "\n",
    "\n",
    "Boolean query Happy AND Sad\n",
    "Answer set rank emojis that has both happy and sad, otherwise, happy then sad, depending on frequency. \n",
    "\n",
    "Tokenization\n",
    "- lowercase might be bad for emojis because we need to keep names apart from words (General Motors)\n",
    "- stemming and lemmatization - Porter algorithm\n",
    "\n",
    "Intersection algorithm for Happy and Sad is O(n+m) where n and m are number of occurrences \n",
    "\n",
    "Tolerant retrieval\n",
    "Wildcard searches like re*val would need to use re AND val. for those searches, \n",
    "k-gram index woudl help\n",
    "phonetic correction\n",
    "lehvenstein distance\n",
    "\n",
    "\n",
    "Index compression\n",
    "Possibly 75% less storage\n",
    "Allow use of caching frequently used terms and \n",
    "Rule of 30 - the 30 most common words account for 30% of the tokens in text. \n",
    "In the postings list, the term is the most space needed. Instead of using the emoji, use a pointer to the emoji\n",
    "\n",
    "\n",
    "Scoring, term weighting, vector space model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Unidecode nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download words corpus if not done before\n",
    "#nltk.download('words')\n",
    "\n",
    "# Set of all English words\n",
    "#english_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\carde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import emoji\n",
    "import glob\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob('data/clean/*.csv')\n",
    "li = []\n",
    "\n",
    "for filename in csv_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from itertools import tee\n",
    "from urllib.parse import urlparse\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "ps = PorterStemmer()\n",
    "tweet = TweetTokenizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "# Save the index to a file\n",
    "\n",
    "\n",
    "def save_index(index, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        # Convert sets to lists before saving\n",
    "        for word in index:\n",
    "            if isinstance(index[word]['emojis'], set):\n",
    "                index[word]['emojis'] = list(index[word]['emojis'])\n",
    "        json.dump(index, f)\n",
    "\n",
    "# useful for debugging\n",
    "def save_index_csv(index, filepath):\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Convert sets to lists before saving\n",
    "        for word in index:\n",
    "            emojis = list(index[word]['emojis'])\n",
    "            writer.writerow([word, index[word]['count'], *emojis])\n",
    "\n",
    "\n",
    "# Clean tweet\n",
    "def clean_text(text: string):\n",
    "    #text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n",
    "                   lambda x: urlparse(x.group()).netloc, text)\n",
    "    ##words = nltk.word_tokenize(text)\n",
    "    words = tweet.tokenize(text)\n",
    "    words = [ps.stem(a) for a in words if a not in stopwords]\n",
    "    return words\n",
    "\n",
    "def index_data(text, index):\n",
    "    \n",
    "    words = clean_text(text)\n",
    "    emoji_iterator = [em for em in emoji.analyze(''.join(words))]\n",
    "    # two independent iters to check if empty in one\n",
    "    iter1, iter2 = tee(emoji_iterator)\n",
    "    try:\n",
    "        _ = next(iter1) \n",
    "        emojis = [em for em in iter2]\n",
    "        char_count = 0\n",
    "        for i, word in enumerate(words):\n",
    "            char_count += len(word)\n",
    "            if word not in index:\n",
    "                index[word] = {'count': 0, 'emojis': []}\n",
    "            index[word]['count'] += 1\n",
    "            for em in emojis:\n",
    "                # Calculate the offset of the emoji from the word\n",
    "                emoji_offset = abs(char_count - em[1].start)\n",
    "                index[word]['emojis'].append({'emoji': em[0], 'offset': emoji_offset})\n",
    "\n",
    "            #index[word]['emojis'] = index[word]['emojis'].union(set(emojis))\n",
    "    except StopIteration:\n",
    "        pass\n",
    "index = defaultdict(dict)\n",
    "df['text'].apply(lambda x: index_data(x, index))\n",
    "\n",
    "save_index(index, 'output/index.json')\n",
    "save_index_csv(index, 'output/index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "a ='aüëã'\n",
    "s = emoji.analyze(a)\n",
    "iter1, iter2 = tee(s)\n",
    "try:\n",
    "    val = next(iter2)\n",
    "except StopIteration:\n",
    "    print(\"awdaw\")\n",
    "for x in iter2:\n",
    "    print(x[1].start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the index from a file\n",
    "def read_index(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        # Convert lists back to sets after loading\n",
    "    for word, info in data.items():\n",
    "        index[word] = {'count': info['count'], 'emojis': []}\n",
    "        for emoji_info in info['emojis']:\n",
    "            index[word]['emojis'].append({'emoji': emoji_info['emoji'], 'offset': emoji_info['offset']})\n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = read_index('index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "search = 'my flight was amazing'\n",
    "\n",
    "collector = defaultdict(dict)\n",
    "#query = clean_text(search)\n",
    "for s in search.split(' '):\n",
    "    q = clean_text(s)\n",
    "    if len(q) <= 0:\n",
    "        continue \n",
    "    else:\n",
    "        q = q[0]\n",
    "    if q not in index:\n",
    "        continue\n",
    "    res = index[q]\n",
    "    query_tf = 1/len(q)\n",
    "    n = len(index)\n",
    "    df_t = len(res['emojis'])\n",
    "    idf_t = math.log(n / df_t)\n",
    "    for r in res['emojis']:\n",
    "        if r['emoji'] not in collector:\n",
    "            collector[r['emoji']] = {'query': s, 'emoji': r, 'score': query_tf * idf_t}\n",
    "        else:\n",
    "            collector[r['emoji']]['score'] += query_tf * idf_t\n",
    "    \n",
    "    # normalize tf_idf on length\n",
    "    # for emoji, info in collector.items():\n",
    "    #     info['score'] /= len(index[emoji]['emojis'])\n",
    "    #     collector[emoji] = info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Split the query into words\n",
    "# query_words = search.split()\n",
    "\n",
    "# for x in range(5):\n",
    "#     res = ''\n",
    "#     for i, word in enumerate(query_words):\n",
    "#         q = clean_text(word)\n",
    "#         if(len(q) <= 0):\n",
    "#             res += word\n",
    "#             continue\n",
    "#         if q[0] in collector:\n",
    "#             emoji = sorted(collector[q[0]]['emojis'], key=lambda x: x['score'], reverse=True)[x]\n",
    "#             print(x, emoji)\n",
    "#             res += word + emoji\n",
    "#     print(res)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top scoring emoji for word 'flight': üò¢\n",
      "Top scoring emoji for word 'amaz': üòä\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "search = 'my flight was amazing'\n",
    "\n",
    "collector = defaultdict(list) \n",
    "query = clean_text(search)\n",
    "queryweight = 1/len(query)\n",
    "\n",
    "for q in query:\n",
    "    if q in index:\n",
    "        res = index[q]\n",
    "        query_tf = queryweight\n",
    "        n = len(index)\n",
    "        df_t = len(res['emojis'])\n",
    "        idf_t = math.log(n / df_t)\n",
    "        for r in res['emojis']:\n",
    "            score = query_tf * idf_t\n",
    "            collector[q].append((r['emoji'], score))\n",
    "\n",
    "# Normalize tf_idf on length and sort by score\n",
    "for word, emojis in collector.items():\n",
    "    normalized_emojis = [(emoji, score / len(index[word]['emojis'])) for emoji, score in emojis]\n",
    "    collector[word] = sorted(normalized_emojis, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top scoring emoji for each word in the search query\n",
    "for word in query:\n",
    "    if word in collector and len(collector[word]) > 0:\n",
    "        print(f\"Top scoring emoji for word '{word}': {collector[word][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_emojis_per_word = {}\n",
    "\n",
    "top_emoji_overall = None\n",
    "top_score_overall = -1\n",
    "\n",
    "for word in search.split(' '):\n",
    "    top_emoji_for_word = None\n",
    "    top_score_for_word = -1\n",
    "\n",
    "    for emoji, info in collector.items():\n",
    "        if info['query'] == word:\n",
    "            if info['score'] > top_score_for_word:\n",
    "                top_emoji_for_word = emoji\n",
    "                top_score_for_word = info['score']\n",
    "            if info['score'] > top_score_overall:\n",
    "                top_emoji_overall = emoji\n",
    "                top_score_overall = info['score']\n",
    "    top_emojis_per_word[word] = top_emoji_for_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\n",
      "flight\n",
      "was\n",
      "amazing\n",
      "top overall üôè\n"
     ]
    }
   ],
   "source": [
    "for x in top_emojis_per_word:\n",
    "    print(x)\n",
    "print(\"top overall\", top_emoji_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
