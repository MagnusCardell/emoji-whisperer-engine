{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a search engine for emojis\n",
    "\n",
    "1. Index the corpus\n",
    "\n",
    "term - token\n",
    "\n",
    "term - emoji index. A sparse matrix with true/false if emoji appears with term\n",
    "inverted index - dictionary of terms, and a list of their appearances (emojis)\n",
    "\n",
    "Building index:\n",
    "1. collect documents (sentences with emojis)\n",
    "2. tokenize the documents\n",
    "3. preprocess the tokens. lowercase, cleanup, english\n",
    "4. Index documents with inverted index\n",
    "\n",
    "Each emoji has unique ID\n",
    "Maintain dictionary and postings\n",
    "dictionary - emoji and pointer to document its from\n",
    "postings - inverted index [emoji, frequency in doc, [docID1, docID2]]\n",
    "\n",
    "\n",
    "Boolean query Happy AND Sad\n",
    "Answer set rank emojis that has both happy and sad, otherwise, happy then sad, depending on frequency. \n",
    "\n",
    "Tokenization\n",
    "- lowercase might be bad for emojis because we need to keep names apart from words (General Motors)\n",
    "- stemming and lemmatization - Porter algorithm\n",
    "\n",
    "Intersection algorithm for Happy and Sad is O(n+m) where n and m are number of occurrences \n",
    "\n",
    "Tolerant retrieval\n",
    "Wildcard searches like re*val would need to use re AND val. for those searches, \n",
    "k-gram index woudl help\n",
    "phonetic correction\n",
    "lehvenstein distance\n",
    "\n",
    "\n",
    "Index compression\n",
    "Possibly 75% less storage\n",
    "Allow use of caching frequently used terms and \n",
    "Rule of 30 - the 30 most common words account for 30% of the tokens in text. \n",
    "In the postings list, the term is the most space needed. Instead of using the emoji, use a pointer to the emoji\n",
    "\n",
    "\n",
    "Scoring, term weighting, vector space model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Unidecode nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download words corpus if not done before\n",
    "#nltk.download('words')\n",
    "\n",
    "# Set of all English words\n",
    "#english_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import emoji\n",
    "import glob\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob('data/clean/*.csv')\n",
    "li = []\n",
    "\n",
    "for filename in csv_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished parsing\n",
      "finished saving json\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from itertools import tee\n",
    "from urllib.parse import urlparse\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "ps = PorterStemmer()\n",
    "tweet = TweetTokenizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "# Save the index to a file\n",
    "\n",
    "\n",
    "def save_index(index, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        # Convert sets to lists before saving\n",
    "        for word in index:\n",
    "            if isinstance(index[word]['emojis'], set):\n",
    "                index[word]['emojis'] = list(index[word]['emojis'])\n",
    "        json.dump(index, f)\n",
    "\n",
    "# useful for debugging\n",
    "def save_index_csv(index, filepath):\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Convert sets to lists before saving\n",
    "        for word in index:\n",
    "            emojis = list(index[word]['emojis'])\n",
    "            writer.writerow([word, index[word]['count'], *emojis])\n",
    "\n",
    "\n",
    "# Clean tweet\n",
    "def clean_text(text: string):\n",
    "    #text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n",
    "                   lambda x: urlparse(x.group()).netloc, text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    #words = tweet.tokenize(text)\n",
    "    words = [ps.stem(a) for a in words if a not in stopwords]\n",
    "    #words = [ps.stem(a) for a in words]\n",
    "    return words\n",
    "\n",
    "def index_data(text, index):\n",
    "    #data = set(text.split(' '))\n",
    "    #data = data.union(set(clean_text(text)))\n",
    "    #data = data.union(set(text.translate(str.maketrans('', '', string.punctuation)).split(' ')))\n",
    "    data = clean_text(text)\n",
    "    emoji_iterator = [em for em in emoji.analyze(\"\".join(data))]\n",
    "    # two independent iters to check if empty in one\n",
    "    iter1, iter2 = tee(emoji_iterator)\n",
    "    try:\n",
    "        _ = next(iter1) \n",
    "        emojis = [em for em in iter2]\n",
    "        char_count = 0\n",
    "        for i, word in enumerate(data):\n",
    "            char_count += len(word)\n",
    "            if word not in index:\n",
    "                index[word] = {'count': 0, 'emojis': []}\n",
    "            index[word]['count'] += 1\n",
    "            for em in emojis:\n",
    "                # Calculate the offset of the emoji from the word\n",
    "                emoji_offset = abs(char_count - em[1].start) + 1\n",
    "                index[word]['emojis'].append({'emoji': em[0], 'offset': emoji_offset})\n",
    "\n",
    "            #index[word]['emojis'] = index[word]['emojis'].union(set(emojis))\n",
    "    except StopIteration:\n",
    "        pass\n",
    "index = defaultdict(dict)\n",
    "df['text'].apply(lambda x: index_data(x, index))\n",
    "print(\"finished parsing\")\n",
    "save_index(index, 'output/index.json')\n",
    "print(\"finished saving json\")\n",
    "#save_index_csv(index, 'output/index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the index from a file\n",
    "def read_index(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        # Convert lists back to sets after loading\n",
    "    for word, info in data.items():\n",
    "        index[word] = {'count': info['count'], 'emojis': []}\n",
    "        for emoji_info in info['emojis']:\n",
    "            index[word]['emojis'].append({'emoji': emoji_info['emoji'], 'offset': emoji_info['offset']})\n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = read_index('output/index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "collector = defaultdict(dict)\n",
    "def search_index(search_query):\n",
    "    collector = defaultdict(dict)\n",
    "    query = clean_text(search_query)\n",
    "    query_weight = 1/len(query)\n",
    "    print(query, query_weight)\n",
    "    for i, q in enumerate(query):\n",
    "        matching_emojis = index[q]\n",
    "\n",
    "        query_tf = query_weight\n",
    "        n = len(index)\n",
    "        df_t = len(matching_emojis['emojis']) if matching_emojis else -1\n",
    "        if df_t == -1:\n",
    "            continue\n",
    "        idf_t = math.log(n / df_t)\n",
    "\n",
    "        emoji_counter = {}\n",
    "        for individual_emoji in matching_emojis['emojis']:\n",
    "            unique_emoji = individual_emoji['emoji']\n",
    "            if unique_emoji not in emoji_counter:\n",
    "                emoji_counter[unique_emoji] = {'count':1, 'offsets':[individual_emoji['offset']]}\n",
    "            else:\n",
    "                emoji_counter[unique_emoji]['count'] += 1\n",
    "                emoji_counter[unique_emoji]['offsets'].append(individual_emoji['offset'])\n",
    "\n",
    "        for unique_emoji, info in emoji_counter.items():\n",
    "            avg_offset = sum(info['offsets']) / len(info['offsets']) if info['offsets'] else 1\n",
    "            if unique_emoji not in collector:\n",
    "                collector[unique_emoji] = {'query': search_query.split(' ')[i], 'raw': q, 'emoji': unique_emoji, 'score': (query_tf * idf_t * info['count']) / avg_offset}\n",
    "            else:\n",
    "                collector[unique_emoji]['score'] += (query_tf * idf_t *  info['count']) / avg_offset\n",
    "    #normalize tf_idf on length\n",
    "    for emoji, info in collector.items():\n",
    "        info['score'] /= index[info['raw']]['count']\n",
    "        collector[emoji] = info\n",
    "    return collector\n",
    "\n",
    "\n",
    "\n",
    "def print_top_emojis(collector, search, n_per_word=3, n_overall=5):\n",
    "    query_words = search.split(' ')\n",
    "    \n",
    "    # Print top emojis for each word\n",
    "    for word in query_words:\n",
    "        word_emojis = [(emoji, info['score']) for emoji, info in collector.items() if info['query'] == word]\n",
    "        word_emojis.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_emojis = word_emojis[:n_per_word]\n",
    "        print(f\"For the word '{word}', the top {n_per_word} emojis are: {top_emojis}\")\n",
    "\n",
    "    # Print top emojis overall\n",
    "    all_emojis = [(emoji, info['score']) for emoji, info in collector.items()]\n",
    "    all_emojis.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_emojis = all_emojis[:n_overall]\n",
    "    print(f\"The top {n_overall} emojis overall are: {top_emojis}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think', 'make', 'insert', 'oper', 'atom'] 0.2\n",
      "For the word 'Think', the top 3 emojis are: [('üòÇ', 0.007103785403500853), ('üôÑ', 0.0015273784317746176), ('üò≠', 0.0015263997989966838)]\n",
      "For the word 'about', the top 3 emojis are: [('üïπ', 3.224485002370002e-05), ('üë®\\u200düîß', 1.576101604642609e-05), ('üîã', 1.161304064956572e-05)]\n",
      "For the word 'how', the top 3 emojis are: [('üë≤üèæ', 0.0006185745590016376), ('3‚É£', 0.0005647854669145387), ('üöé', 0.0002952287667962361)]\n",
      "For the word 'to', the top 3 emojis are: [('üéõ', 0.002968251729093897), ('‚ú¥', 0.0011130943984102113), ('üáßüá≤', 0.0007915337944250392)]\n",
      "For the word 'make', the top 3 emojis are: [('‚ò¢Ô∏è', 0.02422799616142971), ('‚öõ', 0.003461142308775673), ('ü§º\\u200d‚ôÇ', 0.0007125881223949915)]\n",
      "For the word 'your', the top 3 emojis are: []\n",
      "For the word 'insert', the top 3 emojis are: []\n",
      "For the word 'operations', the top 3 emojis are: []\n",
      "For the word 'more', the top 3 emojis are: []\n",
      "For the word 'atomic', the top 3 emojis are: []\n",
      "The top 5 emojis overall are: [('‚ò¢Ô∏è', 0.02422799616142971), ('üòÇ', 0.007103785403500853), ('‚öõ', 0.003461142308775673), ('üéõ', 0.002968251729093897), ('üôÑ', 0.0015273784317746176)]\n"
     ]
    }
   ],
   "source": [
    "query = \"Think about how to make your insert operations more atomic\"\n",
    "collector = search_index(query)\n",
    "# Call the function\n",
    "print_top_emojis(collector, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
