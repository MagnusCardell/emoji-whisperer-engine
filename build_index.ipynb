{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Unidecode nltk emoji pandas autocorrect swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataPreprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def load_data_from_csv(self, filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        self.data.append(df)\n",
    "\n",
    "    def combine_data(self):\n",
    "        self.data = pd.concat(self.data)\n",
    "        self.data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import string\n",
    "class Tokenizer:\n",
    "    def __init__(self, with_stopwords: bool):\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords += list(string.punctuation)\n",
    "        self.punctuation = list(string.punctuation)\n",
    "        self.ps = PorterStemmer()\n",
    "        self.tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=True)\n",
    "        self.cleaner = self._clean_text_withstop if with_stopwords is True else self._clean_text_nostop\n",
    "\n",
    "    def _clean_text_withstop(self, text: str):\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "        words = self.tknzr.tokenize(text)\n",
    "        words = [self.ps.stem(a) for a in words] #if a not in self.punctuation if a not in self.stopwords\n",
    "        return words\n",
    "    def _clean_text_nostop(self, text: str):\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "        words = self.tknzr.tokenize(text)\n",
    "        words = [self.ps.stem(a) for a in words if a not in self.punctuation if a not in self.stopwords] \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import emoji\n",
    "import csv\n",
    "import json\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "class Indexer:\n",
    "\n",
    "    def __init__(self, with_stopwords: bool, group_emojis: bool):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        self.emoji_dict = defaultdict()\n",
    "        self.last_emoji_id = 0\n",
    "        self.tknizer = Tokenizer(with_stopwords)\n",
    "        self.indexer = self.index_data_group_emojis if group_emojis is True else self.index_data_split_groups\n",
    "\n",
    "    def _generate_emoji_id(self, term: str) -> int:\n",
    "        self.last_emoji_id += 1\n",
    "        return self.last_emoji_id\n",
    "\n",
    "    def index_data_group_emojis(self, text: str):\n",
    "        words = self.tknizer.cleaner(text)\n",
    "        # loop backwards\n",
    "        offset = -1\n",
    "        emoji_anchor = ''\n",
    "         # Save any emoji, count each successive word as offset +1\n",
    "        for i in range(len(words)-1,-1,-1):\n",
    "            word = words[i]\n",
    "            if emoji.purely_emoji(word):\n",
    "                # group consecutive emojis\n",
    "                emoji_anchor = emoji.demojize(word) + emoji_anchor if offset <= 1 else emoji.demojize(word)\n",
    "                offset = 0\n",
    "            else:\n",
    "                self.inverted_index.setdefault(word, {'count': 0, 'emojis': {}})\n",
    "                self.inverted_index[word]['count'] += 1\n",
    "\n",
    "                if len(emoji_anchor) > 0:\n",
    "                    if emoji_anchor not in self.emoji_dict:\n",
    "                        self.emoji_dict[emoji_anchor] = self._generate_emoji_id(emoji_anchor)\n",
    "                    emoji_id = self.emoji_dict[emoji_anchor]\n",
    "                    self.inverted_index[word]['emojis'].setdefault(emoji_id, [])\n",
    "                    self.inverted_index[word]['emojis'][emoji_id].append(offset)\n",
    "            offset+=1\n",
    "\n",
    "    def index_data_split_groups(self, text: str):\n",
    "        words = self.tknizer.cleaner(text)\n",
    "        # loop backwards\n",
    "        offset = -1\n",
    "        emoji_anchor = []\n",
    "         # Save any emoji, count each successive word as offset +1\n",
    "        for i in range(len(words)-1,-1,-1):\n",
    "            word = words[i]\n",
    "            if emoji.purely_emoji(word):\n",
    "                # group consecutive emojis\n",
    "                if offset <= 1:\n",
    "                    emoji_anchor.insert(0, emoji.demojize(word))\n",
    "                else:\n",
    "                    emoji_anchor = [emoji.demojize(word)]\n",
    "\n",
    "                offset = 0\n",
    "            elif len(emoji_anchor) > 0:\n",
    "                self.inverted_index.setdefault(word, {'count': 0, 'emojis': {}})\n",
    "                self.inverted_index[word]['count'] += 1\n",
    "\n",
    "                for em in emoji_anchor:\n",
    "                    if em not in self.emoji_dict:\n",
    "                        self.emoji_dict[em] = self._generate_emoji_id(em)\n",
    "                    emoji_id = self.emoji_dict[em]\n",
    "                    self.inverted_index[word]['emojis'].setdefault(emoji_id, [])\n",
    "                    self.inverted_index[word]['emojis'][emoji_id].append(offset)\n",
    "            offset+=1\n",
    "    def _findMedian(self, a):\n",
    "        sorted(a)\n",
    "        n = len(a)\n",
    "        if n % 2 != 0:\n",
    "            return float(a[int(n/2)])\n",
    "    \n",
    "        return float((a[int((n-1)/2)] +\n",
    "                    a[int(n/2)])/2.0)\n",
    "\n",
    "    def save_metadata(self, filepath: str):\n",
    "        flipped_dict = {value: key for key, value in self.emoji_dict.items()}\n",
    "\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(flipped_dict, f)\n",
    "\n",
    "    def save_index(self, filepath: str):\n",
    "        new_dict = {}\n",
    "        for k in self.inverted_index:\n",
    "            new_dict[k] = self.inverted_index[k].copy()\n",
    "            for x in self.inverted_index[k]['emojis']:\n",
    "                new_dict[k]['emojis'][x] = self._findMedian(self.inverted_index[k]['emojis'][x])\n",
    "        with open(f'{filepath}.json', 'w') as f:\n",
    "            json.dump(new_dict, f)\n",
    "        \n",
    "        with open(f'{filepath}.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for word in new_dict:\n",
    "                writer.writerow([word, new_dict[word]['count'], new_dict[word]['emojis']])\n",
    "\n",
    "\n",
    "    def read_index(self, filepath: str):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.inverted_index = json.load(f)\n",
    "    \n",
    "    def read_meta(self, filepath: str):\n",
    "        self.emoji_dict = defaultdict()\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.emoji_dict = json.load(f)\n",
    "\n",
    "    def process_data(self, data):\n",
    "        with Pool(cpu_count()) as p:\n",
    "            p.map(self.index_data, data)\n",
    "\n",
    "# i = Indexer(with_stopwords=False, group_emojis=False)\n",
    "# i.indexer(\"good Good   lucküòÆ‚Äçüí®, good you dawg qhoo . aah üëèüèºüòÆ‚Äçüí®\")\n",
    "# i.save_index(\"output/index\")\n",
    "# i.save_metadata(\"output/meta.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class QueryEngine:\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index.inverted_index\n",
    "        self.meta = index.emoji_dict\n",
    "        self.query_result = defaultdict(dict)\n",
    "\n",
    "    # Vanilla (ish) tf-idf\n",
    "    def process_query_tf_idf(self, search_query, cleaner, n_per_word=3, n_overall=5):\n",
    "        self.query_result = defaultdict(dict)\n",
    "        query = cleaner(search_query)\n",
    "        query_length = len(query)\n",
    "        n = len(self.index)\n",
    "        for i, query_term in enumerate(query):\n",
    "            postings = self.index[query_term] if query_term in self.index else None\n",
    "            if postings is None:\n",
    "                continue\n",
    "            query_weight = 1.0\n",
    "            query_tf = len([q for q in query if q == query_term])\n",
    "            query_weight *= query_tf\n",
    "\n",
    "            df_t = len(postings)\n",
    "            idf_t = math.log(n / df_t)\n",
    "            for emo, median in postings['emojis'].items():\n",
    "                if emo not in self.query_result:\n",
    "                    self.query_result[emo] = {'query': query_term, 'raw': emo,'score': 0}\n",
    "                self.query_result[emo]['score'] += ((query_tf * idf_t) / median) / query_length\n",
    "\n",
    "        all_emojis = [(emoji, info) for emoji, info in self.query_result.items()]\n",
    "        all_emojis.sort(key=lambda x: x[1]['score'], reverse=True)\n",
    "        top_emojis = all_emojis[:n_overall]\n",
    "        return\", \".join(f\"{emoji.emojize(self.meta[emo[0]])}\" for emo in top_emojis) #  {emo[1]['score']:.2f}\n",
    "\n",
    "\n",
    "    def _positional_intersect(self, accumulator, newresults, k):\n",
    "        if accumulator is [] or newresults is None:\n",
    "            return accumulator\n",
    "        \n",
    "        answer = list()\n",
    "        for x_em, x_offsets in accumulator['emojis'].items():\n",
    "            for y_em, y_offsets in newresults['emojis'].items():\n",
    "                if(x_em == y_em):\n",
    "                    answer.append(x_em)\n",
    "        return answer\n",
    "    \n",
    "    def phrase_query(self, search_query, cleaner):\n",
    "        query = cleaner(search_query)\n",
    "        print(query)\n",
    "        results = self.index[query[0]] if query[0] in self.index else None\n",
    "        for term in query[:1]:\n",
    "            matches = self.index[term] if term in self.index else None\n",
    "            results = self._positional_intersect(results, matches, 3)\n",
    "        print(results[:5])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500812\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "csv_files = glob.glob('data/clean/*.csv')\n",
    "\n",
    "for filename in csv_files:\n",
    "    preprocessor.load_data_from_csv(filename)\n",
    "preprocessor.combine_data()\n",
    "preprocessor.data.reset_index(drop=True)\n",
    "\n",
    "print(len(preprocessor.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task stop_group running!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ec82fa8e494274b8eebb0d9cf79d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor done, writing to disk\n",
      "task stop_nogroup running!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b159e123cc8548099bc6748f19474b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor done, writing to disk\n",
      "task nostop_group running!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01967687e98040c5b56468724035d02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor done, writing to disk\n",
      "task nostop_nogroup running!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163ffdf23844412caecc8b108e82b115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor done, writing to disk\n"
     ]
    }
   ],
   "source": [
    "import swifter\n",
    "\n",
    "# Index data\n",
    "def index_data(with_stopwords: bool, group_emojis: bool, filename: str):\n",
    "    print(f'task {filename} running!')\n",
    "    indexer = Indexer(with_stopwords, group_emojis)\n",
    "    preprocessor.data['text'].swifter.apply(lambda x: indexer.indexer(x))\n",
    "    print(\"preprocessor done, writing to disk\")\n",
    "    indexer.save_index(f'output/{filename}')\n",
    "    indexer.save_metadata(f'output/{filename}_meta.json')\n",
    "\n",
    "\n",
    "index_data(True, True, \"stop_group\")\n",
    "index_data(True, False, \"stop_nogroup\")\n",
    "index_data(False, True, \"nostop_group\")\n",
    "index_data(False, False, \"nostop_nogroup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from file\n"
     ]
    }
   ],
   "source": [
    "print(\"reading data from file\")\n",
    "indexer_stop_group = Indexer(True, True)\n",
    "indexer_stop_group.read_index(\"output/stop_group.json\")\n",
    "indexer_stop_group.read_meta(\"output/stop_group_meta.json\")\n",
    "engine_stop_group = QueryEngine(indexer_stop_group)\n",
    "\n",
    "indexer_stop_nogroup = Indexer(True, False)\n",
    "indexer_stop_nogroup.read_index(\"output/stop_nogroup.json\")\n",
    "indexer_stop_nogroup.read_meta(\"output/stop_nogroup_meta.json\")\n",
    "engine_stop_nogroup = QueryEngine(indexer_stop_nogroup)\n",
    "\n",
    "indexer_nostop_group = Indexer(False, True)\n",
    "indexer_nostop_group.read_index(\"output/nostop_group.json\")\n",
    "indexer_nostop_group.read_meta(\"output/nostop_group_meta.json\")\n",
    "engine_nostop_group = QueryEngine(indexer_nostop_group)\n",
    "\n",
    "indexer_nostop_nogroup = Indexer(False, False)\n",
    "indexer_nostop_nogroup.read_index(\"output/nostop_nogroup.json\")\n",
    "indexer_nostop_nogroup.read_meta(\"output/nostop_nogroup_meta.json\")\n",
    "engine_nostop_nogroup = QueryEngine(indexer_nostop_nogroup)\n",
    "\n",
    "#engine.phrase_query(query, indexer._clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëèüèº, üó£Ô∏è, üôåüèª, üòúüëå, üç™\n",
      "1Ô∏è‚É£, üëäüèæ, üîÅ, ü§ù, üë∂üèΩ\n",
      "üôè, üëèüèΩ, üëèüèΩüëèüèΩüëèüèΩ, üëèüèæ, üôåüèºüôåüèºüôåüèº\n",
      "üëèüèæ, üôå, üêê, üòÇ, üëç\n"
     ]
    }
   ],
   "source": [
    "# Query data\n",
    "query = \"Are you ready to take your resume to the next level?\"\n",
    "print(engine_stop_group.process_query_tf_idf(query, indexer_stop_group.tknizer.cleaner))\n",
    "print(engine_stop_nogroup.process_query_tf_idf(query, indexer_stop_nogroup.tknizer.cleaner))\n",
    "print(engine_nostop_group.process_query_tf_idf(query, indexer_nostop_group.tknizer.cleaner))\n",
    "print(engine_nostop_nogroup.process_query_tf_idf(query, indexer_nostop_nogroup.tknizer.cleaner))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I'm feeling happy.\",\n",
    "    \"I'm feeling very sad.\",\n",
    "    \"I'm angry with you.\",\n",
    "    \"I love pizza.\",\n",
    "    \"I dislike broccoli.\",\n",
    "    \"The sunrise this morning was beautiful.\",\n",
    "    \"It's been a long, tiring day.\",\n",
    "    \"I just won the lottery!\",\n",
    "    \"I can't believe we lost the game.\",\n",
    "    \"I'm so excited for the weekend.\",\n",
    "    \"The movie was boring.\",\n",
    "    \"That was the best concert ever!\",\n",
    "    \"I'm scared of spiders.\",\n",
    "    \"My heart is broken.\",\n",
    "    \"I can't wait for my birthday.\",\n",
    "    \"I am feeling so peaceful right now.\",\n",
    "    \"That joke was hilarious.\",\n",
    "    \"I'm feeling pretty indifferent about the whole situation.\",\n",
    "    \"I just got a promotion!\",\n",
    "    \"I feel like crying.\",\n",
    "    \"I can't stand the heat.\",\n",
    "    \"I am freezing!\",\n",
    "    \"That was a delicious meal.\",\n",
    "    \"I am on top of the world!\",\n",
    "    \"I just had a terrible day at work.\",\n",
    "    \"I'm worried about my exam.\",\n",
    "    \"That book was thrilling!\",\n",
    "    \"I'm feeling adventurous.\",\n",
    "    \"I'm feeling so lazy today.\",\n",
    "    \"That was a scary movie.\",\n",
    "    \"I am grateful for my friends.\",\n",
    "    \"The party was a blast!\",\n",
    "    \"That test was really hard.\",\n",
    "    \"I feel loved.\",\n",
    "    \"I feel so rejected.\",\n",
    "    \"I'm bursting with joy.\",\n",
    "    \"I'm disgusted by the trash.\",\n",
    "    \"That was a stressful situation.\",\n",
    "    \"I'm so proud of my team.\",\n",
    "    \"I'm amazed by the view.\",\n",
    "    \"That song was touching.\",\n",
    "    \"I feel so lonely.\",\n",
    "    \"I'm feeling nostalgic.\",\n",
    "    \"The race was intense.\",\n",
    "    \"That was an awkward conversation.\",\n",
    "    \"I feel inspired.\",\n",
    "    \"I'm feeling playful.\",\n",
    "    \"I'm feeling ambitious.\",\n",
    "    \"I'm feeling doubtful.\",\n",
    "    \"That was a surprising result.\",\n",
    "    \"I'm feeling content.\",\n",
    "    \"I'm so disappointed.\",\n",
    "    \"I'm feeling hopeful.\",\n",
    "    \"That was a frustrating experience.\",\n",
    "    \"I feel so appreciated.\",\n",
    "    \"I'm confused.\",\n",
    "    \"I'm feeling motivated.\",\n",
    "    \"I'm feeling pessimistic.\",\n",
    "    \"I'm feeling apathetic.\",\n",
    "    \"That was an impressive performance.\",\n",
    "    \"I'm curious about the result.\",\n",
    "    \"I'm feeling so relaxed.\",\n",
    "    \"I'm feeling agitated.\",\n",
    "    \"That was a depressing story.\",\n",
    "    \"I'm feeling optimistic.\",\n",
    "    \"I feel so empowered.\",\n",
    "    \"I'm feeling ashamed.\",\n",
    "    \"I'm feeling energized.\",\n",
    "    \"I'm feeling apprehensive.\",\n",
    "    \"I'm feeling delighted.\",\n",
    "    \"I'm feeling guilty.\",\n",
    "    \"That was a challenging puzzle.\",\n",
    "    \"I'm feeling so refreshed.\",\n",
    "    \"I'm feeling overwhelmed.\",\n",
    "    \"I'm feeling serene.\",\n",
    "    \"I'm feeling vulnerable.\",\n",
    "    \"That was a fascinating lecture.\",\n",
    "    \"I'm feeling proud.\",\n",
    "    \"I'm feeling humiliated.\",\n",
    "    \"I'm feeling so exhilarated.\",\n",
    "    \"I'm feeling regretful.\",\n",
    "    \"I'm feeling contented.\",\n",
    "    \"I'm feeling restless.\",\n",
    "    \"That was an enchanting evening.\",\n",
    "    \"I'm feeling tranquil.\",\n",
    "    \"I'm feeling tormented.\",\n",
    "    \"I'm feeling triumphant.\",\n",
    "    \"I'm feeling desolate.\",\n",
    "    \"I'm feeling blissful.\",\n",
    "    \"I'm feeling distressed.\",\n",
    "    \"I'm feeling jubilant.\",\n",
    "    \"I'm feeling woeful.\",\n",
    "    \"I'm feeling exuberant.\",\n",
    "    \"I'm feeling despondent.\",\n",
    "    \"I'm feeling ecstatic.\",\n",
    "    \"I'm feeling inconsolable.\",\n",
    "    \"I'm feeling rapturous.\",\n",
    "    \"I'm feeling forlorn.\",\n",
    "    \"I'm feeling exhilarated.\",\n",
    "    \"I'm feeling downhearted.\"\n",
    "]\n",
    "\n",
    "social_media_sentences = [\n",
    "    \"Just had the best coffee at @CafeLuv ‚òïÔ∏è #CoffeeLover\",\n",
    "    \"Getting ready for a Friday night out with the girls! üíÉ #FridayFeeling\",\n",
    "    \"Who else is excited for the new Avengers movie? üçø #MarvelFan\",\n",
    "    \"Can't believe how beautiful the sunset was today. üåÖ #NaturePhotography\",\n",
    "    \"Dinner at my favorite sushi place üç£ #Foodie\",\n",
    "    \"Throwback to my trip to Paris last summer üóº #TravelDiaries\",\n",
    "    \"Feeling so blessed to have such amazing people in my life ü•∞ #Blessed\",\n",
    "    \"Workout done for the day! üí™ #FitnessGoals\",\n",
    "    \"I could spend all day reading at this quiet little bookstore üìö #BookWorm\",\n",
    "    \"Had an awesome time at the concert last night! üé§ #LiveMusic\",\n",
    "    \"I can assist you with booking a flight ‚úàÔ∏è #ChatBot\",\n",
    "    \"What can I help you find today? üîç #CustomerService\",\n",
    "    \"Processing your request now... ‚è≥ #AI\",\n",
    "    \"Your order has been placed! üõçÔ∏è #ShoppingBot\",\n",
    "    \"The weather in New York today is sunny with a high of 75 degrees üåû #WeatherBot\",\n",
    "    \"Directing you to a customer service representative now üìû #HelpBot\",\n",
    "    \"That information is not currently available. Can I assist with anything else? ‚ùì #InfoBot\",\n",
    "    \"You have 3 new notifications üì¨ #ReminderBot\",\n",
    "    \"You successfully completed your daily step goal! üèÉ‚Äç‚ôÄÔ∏è #HealthBot\",\n",
    "    \"Your package has been shipped and is on its way üì¶ #DeliveryUpdate\"\n",
    "]\n",
    "\n",
    "bias_test_sentences = [\n",
    "    \"The strong woman lifted the heavy weights.\",\n",
    "    \"The Muslim man prayed at the mosque.\",\n",
    "    \"She is a successful businesswoman.\",\n",
    "    \"The African American artist created a beautiful painting.\",\n",
    "    \"The gay couple adopted a child.\",\n",
    "    \"He's a Jewish teacher in a high school.\",\n",
    "    \"The nurse worked a long night shift.\",\n",
    "    \"The Asian engineer solved the complex problem.\",\n",
    "    \"The transgender individual voiced their opinion.\",\n",
    "    \"The Latino chef cooked a delicious meal.\",\n",
    "    \"The homeless man found a job.\",\n",
    "    \"The Catholic nun helped the poor.\",\n",
    "    \"The elderly woman is a computer scientist.\",\n",
    "    \"The disabled athlete won the competition.\",\n",
    "    \"The construction worker is a female.\",\n",
    "    \"The Native American lawyer won the case.\",\n",
    "    \"The Hindu priest performed the rituals.\",\n",
    "    \"The single father took care of his children.\",\n",
    "    \"The young man is a ballet dancer.\",\n",
    "    \"The Indian woman is the CEO of the company.\"\n",
    "]\n",
    "\n",
    "name_test_sentences = [\n",
    "    \"Let's meet in New York City.\",\n",
    "    \"I love visiting Paris in the spring.\",\n",
    "    \"I just watched a movie starring Leonardo DiCaprio.\",\n",
    "    \"Did you hear the latest song by Beyonce?\",\n",
    "    \"John is a common name in the United States.\",\n",
    "    \"My best friend's name is Maria.\",\n",
    "    \"Mumbai is known for its delicious street food.\",\n",
    "    \"I just finished reading a book by J.K. Rowling.\",\n",
    "    \"I saw Tom Hanks at the airport today.\",\n",
    "    \"Emily is a popular name for girls.\",\n",
    "    \"I'm traveling to Sydney next month.\",\n",
    "    \"Jennifer Aniston is my favorite actress.\",\n",
    "    \"I attended a concert by the band Coldplay.\",\n",
    "    \"Beijing is a bustling city with a rich history.\",\n",
    "    \"I named my dog after Elon Musk.\",\n",
    "    \"Emma is a beautiful name for a baby girl.\",\n",
    "    \"I enjoyed the book written by Stephen King.\",\n",
    "    \"I'm planning a trip to Tokyo.\",\n",
    "    \"My favorite actor is Will Smith.\",\n",
    "    \"My name is Mohammed.\"\n",
    "]\n",
    "\n",
    "single_word_sentences = [\n",
    "    \"happy\",\n",
    "    \"angry\",\n",
    "    \"love\",\n",
    "    \"hate\",\n",
    "    \"food\",\n",
    "    \"hungry\",\n",
    "    \"tired\",\n",
    "    \"excited\",\n",
    "    \"work\",\n",
    "    \"home\",\n",
    "    \"play\",\n",
    "    \"game\",\n",
    "    \"sports\",\n",
    "    \"music\",\n",
    "    \"movie\",\n",
    "    \"book\",\n",
    "    \"travel\",\n",
    "    \"adventure\",\n",
    "    \"family\",\n",
    "    \"party\"\n",
    "]\n",
    "test_cases = [sentences, social_media_sentences, bias_test_sentences, name_test_sentences, single_word_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = list()\n",
    "\n",
    "for sentence in [sentence for case in test_cases for sentence in case]:\n",
    "    emojis_stop_group = engine_stop_group.process_query_tf_idf(sentence, indexer_stop_group.tknizer.cleaner)\n",
    "    emojis_stop_nogroup = engine_stop_nogroup.process_query_tf_idf(sentence, indexer_stop_nogroup.tknizer.cleaner)\n",
    "    emojis_nostop_group = engine_nostop_group.process_query_tf_idf(sentence, indexer_nostop_group.tknizer.cleaner)\n",
    "    emojis_nostop_nogroup = engine_nostop_nogroup.process_query_tf_idf(sentence, indexer_nostop_nogroup.tknizer.cleaner)\n",
    "    test_results.append({\"sentence\": sentence, \"stop_group\": emojis_stop_group, \"nostop_group\": emojis_nostop_group, \"stop_nogroup\": emojis_stop_nogroup, \"nostop_nogroup\": emojis_nostop_nogroup})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testresults_now.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for result in test_results:\n",
    "        #writer.writerow([result['sentence'], result['group_nostop']])\n",
    "        #writer.writerow(['sg', result['sentence'], result['stop_group'],])\n",
    "        #writer.writerow(['s ', result['sentence'], result['stop_nogroup']])\n",
    "        writer.writerow([' g', result['sentence'], result['nostop_group']])\n",
    "        #writer.writerow(['--', result['sentence'], result['nostop_nogroup']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
