{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Unidecode nltk emoji pandas autocorrect swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataPreprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def load_data_from_csv(self, filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        self.data.append(df)\n",
    "\n",
    "    def combine_data(self):\n",
    "        self.data = pd.concat(self.data)\n",
    "        self.data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import string\n",
    "class Tokenizer:\n",
    "    def __init__(self, with_stopwords: bool):\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords += list(string.punctuation)\n",
    "        self.punctuation = list(string.punctuation)\n",
    "        self.ps = PorterStemmer(\"ORIGINAL_ALGORITHM\")\n",
    "        self.tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=True)\n",
    "        self.cleaner = self._clean_text_withstop if with_stopwords == True else self._clean_text_nostop\n",
    "\n",
    "    def _clean_text_withstop(self, text: str):\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "        words = self.tknzr.tokenize(text)\n",
    "        words = [self.ps.stem(a) for a in words if a[0] != '#'] #if a not in self.punctuation if a not in self.stopwords\n",
    "        return words\n",
    "    def _clean_text_nostop(self, text: str):\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "        words = self.tknzr.tokenize(text)\n",
    "        words = [self.ps.stem(a) for a in words if a not in self.punctuation and a not in self.stopwords and a[0] != '#'] \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import emoji\n",
    "import csv\n",
    "import json\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import math\n",
    "import numpy as np\n",
    "class Indexer:\n",
    "\n",
    "    def __init__(self, with_stopwords: bool, group_emojis: bool):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        self.emoji_dict = defaultdict()\n",
    "        self.emoji_dict_c = defaultdict()\n",
    "        self.last_emoji_id = 0\n",
    "        self.tknizer = Tokenizer(with_stopwords)\n",
    "        self.indexer = self.index_data_group_emojis if group_emojis == True else self.index_data_split_groups\n",
    "\n",
    "    def _generate_emoji_id(self, term: str) -> int:\n",
    "        self.last_emoji_id += 1\n",
    "        return self.last_emoji_id\n",
    "\n",
    "    def index_data_group_emojis(self, text: str):\n",
    "        words = self.tknizer.cleaner(text)\n",
    "        # loop backwards\n",
    "        offset = -1\n",
    "        emoji_anchor = ''\n",
    "         # Save any emoji, count each successive word as offset +1\n",
    "        for i in range(len(words)-1,-1,-1):\n",
    "            word = words[i]\n",
    "            if emoji.purely_emoji(word):\n",
    "                # group consecutive emojis\n",
    "                if offset <= 1:\n",
    "                    emoji_anchor = word + emoji_anchor\n",
    "                    \n",
    "                else:\n",
    "                    emoji_anchor = word\n",
    "                offset = 0\n",
    "\n",
    "            else:\n",
    "                if offset == 1 and len(emoji_anchor)> 0:\n",
    "                    if emoji_anchor not in self.emoji_dict:\n",
    "                        self.emoji_dict[emoji_anchor] = {'id': self._generate_emoji_id(emoji_anchor), 'count': 0}\n",
    "                    self.emoji_dict[emoji_anchor]['count'] += 1\n",
    "\n",
    "                self.inverted_index.setdefault(word, {'count': 0, 'emojis': {}})\n",
    "                self.inverted_index[word]['count'] += 1\n",
    "\n",
    "                if len(emoji_anchor) > 0:\n",
    "                    emoji_id = self.emoji_dict[emoji_anchor]['id']\n",
    "                    self.inverted_index[word]['emojis'].setdefault(emoji_id, [])\n",
    "                    self.inverted_index[word]['emojis'][emoji_id].append(offset)\n",
    "            offset+=1\n",
    "\n",
    "    def index_data_split_groups(self, text: str):\n",
    "        words = self.tknizer.cleaner(text)\n",
    "        # loop backwards\n",
    "        offset = -1\n",
    "        emoji_anchor = []\n",
    "         # Save any emoji, count each successive word as offset +1\n",
    "        for i in range(len(words)-1,-1,-1):\n",
    "            word = words[i]\n",
    "            if emoji.purely_emoji(word):\n",
    "                # group consecutive emojis\n",
    "                if offset <= 1:\n",
    "                    emoji_anchor.insert(0, word)\n",
    "                else:\n",
    "                    emoji_anchor = [word]\n",
    "\n",
    "                offset = 0\n",
    "                if word not in self.emoji_dict:\n",
    "                    self.emoji_dict[word] = {'id': self._generate_emoji_id(word), 'count': 0}\n",
    "                self.emoji_dict[word]['count'] += 1\n",
    "\n",
    "            elif len(emoji_anchor) > 0:\n",
    "                self.inverted_index.setdefault(word, {'count': 0, 'emojis': {}})\n",
    "                self.inverted_index[word]['count'] += 1\n",
    "\n",
    "                for em in emoji_anchor:\n",
    "                    emoji_id = self.emoji_dict[em]['id']\n",
    "                    self.inverted_index[word]['emojis'].setdefault(emoji_id, [])\n",
    "                    self.inverted_index[word]['emojis'][emoji_id].append(offset)\n",
    "            offset+=1\n",
    "    def _findMedian(self, a):\n",
    "        sorted(a)\n",
    "        n = len(a)\n",
    "        if n % 2 != 0:\n",
    "            return float(a[int(n/2)])\n",
    "    \n",
    "        return float((a[int((n-1)/2)] +\n",
    "                    a[int(n/2)])/2.0)\n",
    "    \n",
    "    def _precompute_score(self, a, idf_t, emoji_frequency):\n",
    "        query_tf = 1\n",
    "        median = self._findMedian(a)\n",
    "        score = (query_tf * idf_t) / (median + emoji_frequency)\n",
    "        return round(score, 2)\n",
    "\n",
    "    def save_metadata(self, flipped_dict, filepath: str):\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(flipped_dict, f)\n",
    "        return flipped_dict\n",
    "\n",
    "    def save_index(self, filepath: str):\n",
    "        flipped_dict = {value['id']: {'emoji': key, 'count': value['count']} for key, value in self.emoji_dict.items()}\n",
    "        new_dict = {}\n",
    "        n = len(self.inverted_index)\n",
    "        for k in list(self.inverted_index.keys()):\n",
    "            new_dict[k] = self.inverted_index[k].copy()\n",
    "            df_t = len(self.inverted_index[k])\n",
    "            idf_t = math.log(n / df_t)\n",
    "            for x in list(self.inverted_index[k]['emojis'].keys()):\n",
    "                emoji_frequency = flipped_dict[x]['count'] / len(self.emoji_dict)\n",
    "                score = self._precompute_score(self.inverted_index[k]['emojis'][x], idf_t, emoji_frequency)\n",
    "                new_dict[k]['emojis'][x] = score\n",
    "                \n",
    "            scores = list(new_dict[k]['emojis'].values())\n",
    "            del new_dict[k]['count']\n",
    "            if len(scores) == 0:\n",
    "                del new_dict[k]\n",
    "            else:\n",
    "                top_score = max(scores)\n",
    "                std_dev_score = np.std(scores)\n",
    "                for key in list(new_dict[k]['emojis'].keys()):\n",
    "                    if new_dict[k]['emojis'][key] < (top_score - std_dev_score):\n",
    "                        del new_dict[k]['emojis'][key]\n",
    "\n",
    "\n",
    "        flipped_dict = {key: value['emoji'] for key, value in flipped_dict.items()}\n",
    "        self.last_emoji_id = 0\n",
    "        compressed_dict = {}\n",
    "        for k in list(flipped_dict.keys()):\n",
    "            emoji_set = flipped_dict[k]\n",
    "            emoji_key = ''\n",
    "            for e in emoji_set:\n",
    "                if e not in compressed_dict:\n",
    "                    e_id = self._generate_emoji_id(e)\n",
    "                    compressed_dict[e] = str(e_id)\n",
    "\n",
    "                if emoji_key != '':\n",
    "                    emoji_key += ','\n",
    "                emoji_key += compressed_dict[e]\n",
    "            flipped_dict[k] = emoji_key\n",
    "\n",
    "        # for k in list(new_dict.keys()):\n",
    "        #     for x in list(new_dict[k]['emojis'].keys()):\n",
    "        #         em_key = flipped_dict[x]\n",
    "        #         new_dict[k]['emojis'][em_key] = new_dict[k]['emojis'][x]\n",
    "        #         del new_dict[k]['emojis'][x]\n",
    "\n",
    "        with open(f'{filepath}.json', 'w') as f:\n",
    "            json.dump(new_dict, f)\n",
    "        \n",
    "        with open(f'{filepath}.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for word in new_dict:\n",
    "                writer.writerow([word, new_dict[word]['emojis']])\n",
    "\n",
    "        self.save_metadata(flipped_dict, f\"{filepath}_meta.json\")\n",
    "\n",
    "        flipped_compressed = {value: key for key, value in compressed_dict.items()}\n",
    "        self.save_metadata(flipped_compressed, f\"{filepath}_meta_c.json\")\n",
    "\n",
    "\n",
    "    def read_index(self, filepath: str):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.inverted_index = json.load(f)\n",
    "    \n",
    "    def read_meta(self, filepath: str):\n",
    "        self.emoji_dict = defaultdict()\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.emoji_dict = json.load(f)\n",
    "    \n",
    "    def read_compressed(self, filepath: str):\n",
    "        self.emoji_dict_c = defaultdict()\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.emoji_dict_c = json.load(f)\n",
    "\n",
    "    def process_data(self, data):\n",
    "        with Pool(cpu_count()) as p:\n",
    "            p.map(self.index_data, data)\n",
    "\n",
    "i = Indexer(with_stopwords=False, group_emojis=True)\n",
    "i.indexer(\"good Good in with you a    lucküòÆ‚Äçüí®, good #you dawg qhoo . aah üëèüèºüòÆ‚Äçüí®\")\n",
    "i.save_index(\"output/index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryEngine:\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index.inverted_index\n",
    "        self.meta = index.emoji_dict\n",
    "        self.comp = index.emoji_dict_c\n",
    "        self.query_result = defaultdict(dict)\n",
    "\n",
    "    def process_query_score(self, search_query, cleaner, n_per_word=3, n_overall=5):\n",
    "        self.query_result = defaultdict(dict)\n",
    "        query = cleaner(search_query)\n",
    "        query_length = len(query)\n",
    "        for i, query_term in enumerate(query):\n",
    "            postings = self.index[query_term] if query_term in self.index else None\n",
    "            if postings is None:\n",
    "                continue\n",
    "           \n",
    "            for emo, score in postings['emojis'].items():\n",
    "                if emo not in self.query_result:\n",
    "                    self.query_result[emo] = {'query': query_term, 'raw': emo,'score': 0}\n",
    "                self.query_result[emo]['score'] += score / query_length\n",
    "\n",
    "        all_emojis = [(emoji, info) for emoji, info in self.query_result.items()]\n",
    "        all_emojis.sort(key=lambda x: x[1]['score'], reverse=True)\n",
    "        top_emojis = all_emojis[:n_overall]\n",
    "\n",
    "        result = ''\n",
    "        for emo in top_emojis:\n",
    "            emoji_set = self.meta[ emo[0] ]\n",
    "            for e in emoji_set.split(','):\n",
    "                result += emoji.emojize(self.comp[ e ])\n",
    "            result += ','\n",
    "        return result\n",
    "        #return\", \".join(f\"{emoji.emojize(self.meta[emo[0]])}\" for emo in top_emojis) #  {emo[1]['score']:.2f}\n",
    "\n",
    "\n",
    "    def _positional_intersect(self, accumulator, newresults, k):\n",
    "        if accumulator is [] or newresults is None:\n",
    "            return accumulator\n",
    "        \n",
    "        answer = list()\n",
    "        for x_em, x_offsets in accumulator['emojis'].items():\n",
    "            for y_em, y_offsets in newresults['emojis'].items():\n",
    "                if(x_em == y_em):\n",
    "                    answer.append(x_em)\n",
    "        return answer\n",
    "    \n",
    "    def phrase_query(self, search_query, cleaner):\n",
    "        query = cleaner(search_query)\n",
    "        results = self.index[query[0]] if query[0] in self.index else None\n",
    "        for term in query[:1]:\n",
    "            matches = self.index[term] if term in self.index else None\n",
    "            results = self._positional_intersect(results, matches, 3)\n",
    "        print(results[:5])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518536\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "csv_files = glob.glob('data/clean/*.csv')\n",
    "\n",
    "for filename in csv_files:\n",
    "    preprocessor.load_data_from_csv(filename)\n",
    "preprocessor.combine_data()\n",
    "preprocessor.data.reset_index(drop=True)\n",
    "\n",
    "print(len(preprocessor.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task nostop_group running!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3377d93f5114228963488cfd2c1cccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/518536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor done, writing to disk\n"
     ]
    }
   ],
   "source": [
    "import swifter\n",
    "\n",
    "# Index data\n",
    "def index_data(with_stopwords: bool, group_emojis: bool, filename: str):\n",
    "    print(f'task {filename} running!')\n",
    "    indexer = Indexer(with_stopwords, group_emojis)\n",
    "    preprocessor.data['text'].swifter.apply(lambda x: indexer.indexer(x))\n",
    "    print(\"preprocessor done, writing to disk\")\n",
    "    indexer.save_index(f'output/{filename}')\n",
    "\n",
    "\n",
    "#index_data(True, True, \"stop_group\")\n",
    "#index_data(True, False, \"stop_nogroup\")\n",
    "index_data(False, True, \"nostop_group\")\n",
    "#index_data(False, False, \"nostop_nogroup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from file\n"
     ]
    }
   ],
   "source": [
    "print(\"reading data from file\")\n",
    "# indexer_stop_group = Indexer(True, True)\n",
    "# indexer_stop_group.read_index(\"output/stop_group.json\")\n",
    "# indexer_stop_group.read_meta(\"output/stop_group_meta.json\")\n",
    "# engine_stop_group = QueryEngine(indexer_stop_group)\n",
    "\n",
    "# indexer_stop_nogroup = Indexer(True, False)\n",
    "# indexer_stop_nogroup.read_index(\"output/stop_nogroup.json\")\n",
    "# indexer_stop_nogroup.read_meta(\"output/stop_nogroup_meta.json\")\n",
    "# engine_stop_nogroup = QueryEngine(indexer_stop_nogroup)\n",
    "\n",
    "indexer_nostop_group = Indexer(False, True)\n",
    "indexer_nostop_group.read_index(\"output/nostop_group.json\")\n",
    "indexer_nostop_group.read_meta(\"output/nostop_group_meta.json\")\n",
    "indexer_nostop_group.read_compressed(\"output/nostop_group_meta_c.json\")\n",
    "engine_nostop_group = QueryEngine(indexer_nostop_group)\n",
    "\n",
    "# indexer_nostop_nogroup = Indexer(False, False)\n",
    "# indexer_nostop_nogroup.read_index(\"output/nostop_nogroup.json\")\n",
    "# indexer_nostop_nogroup.read_meta(\"output/nostop_nogroup_meta.json\")\n",
    "# engine_nostop_nogroup = QueryEngine(indexer_nostop_nogroup)\n",
    "\n",
    "#engine.phrase_query(query, indexer._clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Query data\n",
    "query = \"off\"\n",
    "#print(engine_stop_group.process_query_score(query, indexer_stop_group.tknizer.cleaner))\n",
    "#print(engine_stop_nogroup.process_query_score(query, indexer_stop_nogroup.tknizer.cleaner))\n",
    "print(engine_nostop_group.process_query_score(query, indexer_nostop_group.tknizer.cleaner))\n",
    "#print(engine_nostop_nogroup.process_query_score(query, indexer_nostop_nogroup.tknizer.cleaner))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I'm feeling happy.\",\n",
    "    \"I'm feeling very sad.\",\n",
    "    \"I'm angry with you.\",\n",
    "    \"I love pizza.\",\n",
    "    \"I dislike broccoli.\",\n",
    "    \"The sunrise this morning was beautiful.\",\n",
    "    \"It's been a long, tiring day.\",\n",
    "    \"I just won the lottery!\",\n",
    "    \"I can't believe we lost the game.\",\n",
    "    \"I'm so excited for the weekend.\",\n",
    "    \"The movie was boring.\",\n",
    "    \"That was the best concert ever!\",\n",
    "    \"I'm scared of spiders.\",\n",
    "    \"My heart is broken.\",\n",
    "    \"I can't wait for my birthday.\",\n",
    "    \"I am feeling so peaceful right now.\",\n",
    "    \"That joke was hilarious.\",\n",
    "    \"I'm feeling pretty indifferent about the whole situation.\",\n",
    "    \"I just got a promotion!\",\n",
    "    \"I feel like crying.\",\n",
    "    \"I can't stand the heat.\",\n",
    "    \"I am freezing!\",\n",
    "    \"That was a delicious meal.\",\n",
    "    \"I am on top of the world!\",\n",
    "    \"I just had a terrible day at work.\",\n",
    "    \"I'm worried about my exam.\",\n",
    "    \"That book was thrilling!\",\n",
    "    \"I'm feeling adventurous.\",\n",
    "    \"I'm feeling so lazy today.\",\n",
    "    \"That was a scary movie.\",\n",
    "    \"I am grateful for my friends.\",\n",
    "    \"The party was a blast!\",\n",
    "    \"That test was really hard.\",\n",
    "    \"I feel loved.\",\n",
    "    \"I feel so rejected.\",\n",
    "    \"I'm bursting with joy.\",\n",
    "    \"I'm disgusted by the trash.\",\n",
    "    \"That was a stressful situation.\",\n",
    "    \"I'm so proud of my team.\",\n",
    "    \"I'm amazed by the view.\",\n",
    "    \"That song was touching.\",\n",
    "    \"I feel so lonely.\",\n",
    "    \"I'm feeling nostalgic.\",\n",
    "    \"The race was intense.\",\n",
    "    \"That was an awkward conversation.\",\n",
    "    \"I feel inspired.\",\n",
    "    \"I'm feeling playful.\",\n",
    "    \"I'm feeling ambitious.\",\n",
    "    \"I'm feeling doubtful.\",\n",
    "    \"That was a surprising result.\",\n",
    "    \"I'm feeling content.\",\n",
    "    \"I'm so disappointed.\",\n",
    "    \"I'm feeling hopeful.\",\n",
    "    \"That was a frustrating experience.\",\n",
    "    \"I feel so appreciated.\",\n",
    "    \"I'm confused.\",\n",
    "    \"I'm feeling motivated.\",\n",
    "    \"I'm feeling pessimistic.\",\n",
    "    \"I'm feeling apathetic.\",\n",
    "    \"That was an impressive performance.\",\n",
    "    \"I'm curious about the result.\",\n",
    "    \"I'm feeling so relaxed.\",\n",
    "    \"I'm feeling agitated.\",\n",
    "    \"That was a depressing story.\",\n",
    "    \"I'm feeling optimistic.\",\n",
    "    \"I feel so empowered.\",\n",
    "    \"I'm feeling ashamed.\",\n",
    "    \"I'm feeling energized.\",\n",
    "    \"I'm feeling apprehensive.\",\n",
    "    \"I'm feeling delighted.\",\n",
    "    \"I'm feeling guilty.\",\n",
    "    \"That was a challenging puzzle.\",\n",
    "    \"I'm feeling so refreshed.\",\n",
    "    \"I'm feeling overwhelmed.\",\n",
    "    \"I'm feeling serene.\",\n",
    "    \"I'm feeling vulnerable.\",\n",
    "    \"That was a fascinating lecture.\",\n",
    "    \"I'm feeling proud.\",\n",
    "    \"I'm feeling humiliated.\",\n",
    "    \"I'm feeling so exhilarated.\",\n",
    "    \"I'm feeling regretful.\",\n",
    "    \"I'm feeling contented.\",\n",
    "    \"I'm feeling restless.\",\n",
    "    \"That was an enchanting evening.\",\n",
    "    \"I'm feeling tranquil.\",\n",
    "    \"I'm feeling tormented.\",\n",
    "    \"I'm feeling triumphant.\",\n",
    "    \"I'm feeling desolate.\",\n",
    "    \"I'm feeling blissful.\",\n",
    "    \"I'm feeling distressed.\",\n",
    "    \"I'm feeling jubilant.\",\n",
    "    \"I'm feeling woeful.\",\n",
    "    \"I'm feeling exuberant.\",\n",
    "    \"I'm feeling despondent.\",\n",
    "    \"I'm feeling ecstatic.\",\n",
    "    \"I'm feeling inconsolable.\",\n",
    "    \"I'm feeling rapturous.\",\n",
    "    \"I'm feeling forlorn.\",\n",
    "    \"I'm feeling exhilarated.\",\n",
    "    \"I'm feeling downhearted.\"\n",
    "]\n",
    "\n",
    "social_media_sentences = [\n",
    "    \"Just had the best coffee at @CafeLuv #CoffeeLover\",\n",
    "    \"Getting ready for a Friday night out with the girls! #FridayFeeling\",\n",
    "    \"Who else is excited for the new Avengers movie? #MarvelFan\",\n",
    "    \"Can't believe how beautiful the sunset was today. #NaturePhotography\",\n",
    "    \"Dinner at my favorite sushi place #Foodie\",\n",
    "    \"Throwback to my trip to Paris last summer #TravelDiaries\",\n",
    "    \"Feeling so blessed to have such amazing people in my life #Blessed\",\n",
    "    \"Workout done for the day! #FitnessGoals\",\n",
    "    \"I could spend all day reading at this quiet little bookstore #BookWorm\",\n",
    "    \"Had an awesome time at the concert last night! #LiveMusic\",\n",
    "    \"I can assist you with booking a flight #ChatBot\",\n",
    "    \"What can I help you find today? #CustomerService\",\n",
    "    \"Processing your request now... #AI\",\n",
    "    \"Your order has been placed! #ShoppingBot\",\n",
    "    \"The weather in New York today is sunny with a high of 75 degrees #WeatherBot\",\n",
    "    \"Directing you to a customer service representative now #HelpBot\",\n",
    "    \"That information is not currently available. Can I assist with anything else? #InfoBot\",\n",
    "    \"You have 3 new notifications #ReminderBot\",\n",
    "    \"You successfully completed your daily step goal! #HealthBot\",\n",
    "    \"Your package has been shipped and is on its way #DeliveryUpdate\"\n",
    "]\n",
    "\n",
    "bias_test_sentences = [\n",
    "    \"The strong woman lifted the heavy weights.\",\n",
    "    \"The Muslim man prayed at the mosque.\",\n",
    "    \"She is a successful businesswoman.\",\n",
    "    \"The African American artist created a beautiful painting.\",\n",
    "    \"The gay couple adopted a child.\",\n",
    "    \"He's a Jewish teacher in a high school.\",\n",
    "    \"The nurse worked a long night shift.\",\n",
    "    \"The Asian engineer solved the complex problem.\",\n",
    "    \"The transgender individual voiced their opinion.\",\n",
    "    \"The Latino chef cooked a delicious meal.\",\n",
    "    \"The homeless man found a job.\",\n",
    "    \"The Catholic nun helped the poor.\",\n",
    "    \"The elderly woman is a computer scientist.\",\n",
    "    \"The disabled athlete won the competition.\",\n",
    "    \"The construction worker is a female.\",\n",
    "    \"The Native American lawyer won the case.\",\n",
    "    \"The Hindu priest performed the rituals.\",\n",
    "    \"The single father took care of his children.\",\n",
    "    \"The young man is a ballet dancer.\",\n",
    "    \"The Indian woman is the CEO of the company.\"\n",
    "]\n",
    "\n",
    "name_test_sentences = [\n",
    "    \"Let's meet in New York City.\",\n",
    "    \"I love visiting Paris in the spring.\",\n",
    "    \"I just watched a movie starring Leonardo DiCaprio.\",\n",
    "    \"Did you hear the latest song by Beyonce?\",\n",
    "    \"John is a common name in the United States.\",\n",
    "    \"My best friend's name is Maria.\",\n",
    "    \"Mumbai is known for its delicious street food.\",\n",
    "    \"I just finished reading a book by J.K. Rowling.\",\n",
    "    \"I saw Tom Hanks at the airport today.\",\n",
    "    \"Emily is a popular name for girls.\",\n",
    "    \"I'm traveling to Sydney next month.\",\n",
    "    \"Jennifer Aniston is my favorite actress.\",\n",
    "    \"I attended a concert by the band Coldplay.\",\n",
    "    \"Beijing is a bustling city with a rich history.\",\n",
    "    \"I named my dog after Elon Musk.\",\n",
    "    \"Emma is a beautiful name for a baby girl.\",\n",
    "    \"I enjoyed the book written by Stephen King.\",\n",
    "    \"I'm planning a trip to Tokyo.\",\n",
    "    \"My favorite actor is Will Smith.\",\n",
    "    \"My name is Mohammed.\"\n",
    "]\n",
    "\n",
    "single_word_sentences = [\n",
    "    \"happy\",\n",
    "    \"angry\",\n",
    "    \"love\",\n",
    "    \"hate\",\n",
    "    \"food\",\n",
    "    \"hungry\",\n",
    "    \"tired\",\n",
    "    \"excited\",\n",
    "    \"work\",\n",
    "    \"home\",\n",
    "    \"play\",\n",
    "    \"game\",\n",
    "    \"sports\",\n",
    "    \"music\",\n",
    "    \"movie\",\n",
    "    \"book\",\n",
    "    \"travel\",\n",
    "    \"adventure\",\n",
    "    \"family\",\n",
    "    \"party\"\n",
    "]\n",
    "test_cases = [sentences, social_media_sentences, bias_test_sentences, name_test_sentences, single_word_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = list()\n",
    "\n",
    "for sentence in [sentence for case in test_cases for sentence in case]:\n",
    "    #emojis_stop_group = engine_stop_group.process_query_score(sentence, indexer_stop_group.tknizer.cleaner)\n",
    "    #emojis_stop_nogroup = engine_stop_nogroup.process_query_score(sentence, indexer_stop_nogroup.tknizer.cleaner)\n",
    "    emojis_nostop_group = engine_nostop_group.process_query_score(sentence, indexer_nostop_group.tknizer.cleaner)\n",
    "    #emojis_nostop_nogroup = engine_nostop_nogroup.process_query_score(sentence, indexer_nostop_nogroup.tknizer.cleaner)\n",
    "    #test_results.append({\"sentence\": sentence, \"stop_group\": emojis_stop_group, \"nostop_group\": emojis_nostop_group, \"stop_nogroup\": emojis_stop_nogroup, \"nostop_nogroup\": emojis_nostop_nogroup})\n",
    "    test_results.append({\"sentence\": sentence, \"nostop_group\": emojis_nostop_group})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testresults_now.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for result in test_results:\n",
    "        #writer.writerow(['sg', result['sentence'], result['stop_group'],])\n",
    "        #writer.writerow(['s ', result['sentence'], result['stop_nogroup']])\n",
    "        writer.writerow([' g', result['sentence'], result['nostop_group']])\n",
    "        #writer.writerow(['--', result['sentence'], result['nostop_nogroup']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "type",
     "evalue": "name 'indexer_stop_group' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ex \u001b[39m=\u001b[39m indexer_stop_group\u001b[39m.\u001b[39minverted_index[\u001b[39m'\u001b[39m\u001b[39mdog\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m sorted_ex \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(ex[\u001b[39m'\u001b[39m\u001b[39memojis\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m item: item[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m em, score \u001b[39min\u001b[39;00m sorted_ex:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'indexer_stop_group' is not defined"
     ]
    }
   ],
   "source": [
    "ex = indexer_stop_group.inverted_index['dog']\n",
    "sorted_ex = sorted(ex['emojis'].items(), key=lambda item: item[1], reverse=False)\n",
    "for em, score in sorted_ex:\n",
    "        print(f\"Emoji: {indexer_stop_group.emoji_dict[em]}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëèüèºüòÆ‚Äçüí®,üòÆ‚Äçüí®,\n"
     ]
    }
   ],
   "source": [
    "i = Indexer(False, False)\n",
    "i.read_index(\"output/index.json\")\n",
    "i.read_meta(\"output/index_meta.json\")\n",
    "i.read_compressed(\"output/index_meta_c.json\")\n",
    "\n",
    "e = QueryEngine(i)\n",
    "\n",
    "emojis_nostop_group = e.process_query_score(\"aah good\", i.tknizer.cleaner)\n",
    "\n",
    "print(emojis_nostop_group)\n",
    "# ex = i.inverted_index['good']\n",
    "# sorted_ex = sorted(ex['emojis'].items(), key=lambda item: item[1], reverse=False)\n",
    "# for emoji, score in sorted_ex:\n",
    "#         print(f\"Emoji: {i.emoji_dict[emoji]}, Score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
