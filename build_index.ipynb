{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Unidecode nltk emoji pandas autocorrect swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataPreprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def load_data_from_csv(self, filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        self.data.append(df)\n",
    "\n",
    "    def combine_data(self):\n",
    "        self.data = pd.concat(self.data)\n",
    "        self.data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import string\n",
    "class Tokenizer:\n",
    "    def __init__(self, with_stopwords: bool):\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords += list(string.punctuation)\n",
    "        self.punctuation = list(string.punctuation)\n",
    "        self.ps = PorterStemmer(\"ORIGINAL_ALGORITHM\")\n",
    "        self.tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=True)\n",
    "        self.cleaner = self._clean_text_withstop if with_stopwords == True else self._clean_text_nostop\n",
    "\n",
    "    def _clean_text_withstop(self, text: str):\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "        words = self.tknzr.tokenize(text)\n",
    "        words = [self.ps.stem(a) for a in words if a[0] != '#'] #if a not in self.punctuation if a not in self.stopwords\n",
    "        return words\n",
    "    def _clean_text_nostop(self, text: str):\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "        words = self.tknzr.tokenize(text)\n",
    "        words = [self.ps.stem(a) for a in words if a not in self.punctuation and a not in self.stopwords and a[0] != '#'] \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import emoji\n",
    "import csv\n",
    "import json\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import math\n",
    "import numpy as np\n",
    "class Indexer:\n",
    "\n",
    "    def __init__(self, with_stopwords: bool, group_emojis: bool):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        self.emoji_dict = defaultdict()\n",
    "        self.last_emoji_id = 0\n",
    "        self.tknizer = Tokenizer(with_stopwords)\n",
    "        self.indexer = self.index_data_group_emojis if group_emojis == True else self.index_data_split_groups\n",
    "\n",
    "    def _generate_emoji_id(self, term: str) -> int:\n",
    "        self.last_emoji_id += 1\n",
    "        return self.last_emoji_id\n",
    "\n",
    "    def index_data_group_emojis(self, text: str):\n",
    "        words = self.tknizer.cleaner(text)\n",
    "        # loop backwards\n",
    "        offset = -1\n",
    "        emoji_anchor = ''\n",
    "         # Save any emoji, count each successive word as offset +1\n",
    "        for i in range(len(words)-1,-1,-1):\n",
    "            word = words[i]\n",
    "            if emoji.purely_emoji(word):\n",
    "                # group consecutive emojis\n",
    "                if offset <= 1:\n",
    "                    emoji_anchor = word + emoji_anchor\n",
    "                    \n",
    "                else:\n",
    "                    emoji_anchor = word\n",
    "                offset = 0\n",
    "\n",
    "            else:\n",
    "                if offset == 1 and len(emoji_anchor)> 0:\n",
    "                    if emoji_anchor not in self.emoji_dict:\n",
    "                        self.emoji_dict[emoji_anchor] = {'id': self._generate_emoji_id(emoji_anchor), 'count': 0}\n",
    "                    self.emoji_dict[emoji_anchor]['count'] += 1\n",
    "                    \n",
    "                self.inverted_index.setdefault(word, {'count': 0, 'emojis': {}})\n",
    "                self.inverted_index[word]['count'] += 1\n",
    "\n",
    "                if len(emoji_anchor) > 0:\n",
    "                    emoji_id = self.emoji_dict[emoji_anchor]['id']\n",
    "                    self.inverted_index[word]['emojis'].setdefault(emoji_id, [])\n",
    "                    self.inverted_index[word]['emojis'][emoji_id].append(offset)\n",
    "            offset+=1\n",
    "\n",
    "    def index_data_split_groups(self, text: str):\n",
    "        words = self.tknizer.cleaner(text)\n",
    "        # loop backwards\n",
    "        offset = -1\n",
    "        emoji_anchor = []\n",
    "         # Save any emoji, count each successive word as offset +1\n",
    "        for i in range(len(words)-1,-1,-1):\n",
    "            word = words[i]\n",
    "            if emoji.purely_emoji(word):\n",
    "                # group consecutive emojis\n",
    "                if offset <= 1:\n",
    "                    emoji_anchor.insert(0, word)\n",
    "                else:\n",
    "                    emoji_anchor = [word]\n",
    "\n",
    "                offset = 0\n",
    "                if word not in self.emoji_dict:\n",
    "                    self.emoji_dict[word] = {'id': self._generate_emoji_id(word), 'count': 0}\n",
    "                self.emoji_dict[word]['count'] += 1\n",
    "\n",
    "            elif len(emoji_anchor) > 0:\n",
    "                self.inverted_index.setdefault(word, {'count': 0, 'emojis': {}})\n",
    "                self.inverted_index[word]['count'] += 1\n",
    "\n",
    "                for em in emoji_anchor:\n",
    "                    emoji_id = self.emoji_dict[em]['id']\n",
    "                    self.inverted_index[word]['emojis'].setdefault(emoji_id, [])\n",
    "                    self.inverted_index[word]['emojis'][emoji_id].append(offset)\n",
    "            offset+=1\n",
    "    def _findMedian(self, a):\n",
    "        sorted(a)\n",
    "        n = len(a)\n",
    "        if n % 2 != 0:\n",
    "            return float(a[int(n/2)])\n",
    "    \n",
    "        return float((a[int((n-1)/2)] +\n",
    "                    a[int(n/2)])/2.0)\n",
    "    \n",
    "    def _precompute_score(self, a, idf_t, emoji_frequency):\n",
    "        query_tf = 1\n",
    "        median = self._findMedian(a)\n",
    "        score = (query_tf * idf_t) / (median + emoji_frequency)\n",
    "        return round(score, 2)\n",
    "\n",
    "    def save_metadata(self, filepath: str):\n",
    "        flipped_dict = {value['id']: {'emoji': key, 'count': value['count']} for key, value in self.emoji_dict.items()}\n",
    "\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(flipped_dict, f)\n",
    "        return flipped_dict\n",
    "\n",
    "    def save_index(self, filepath: str):\n",
    "        flipped_dict = self.save_metadata(f\"{filepath}_meta.json\")\n",
    "        new_dict = {}\n",
    "        n = len(self.inverted_index)\n",
    "        for k in list(self.inverted_index.keys()):\n",
    "            new_dict[k] = self.inverted_index[k].copy()\n",
    "            df_t = len(self.inverted_index[k])\n",
    "            idf_t = math.log(n / df_t)\n",
    "            for x in list(self.inverted_index[k]['emojis'].keys()):\n",
    "                emoji_frequency = flipped_dict[x]['count'] / len(self.emoji_dict)\n",
    "                score = self._precompute_score(self.inverted_index[k]['emojis'][x], idf_t, emoji_frequency)\n",
    "                new_dict[k]['emojis'][x] = score\n",
    "                \n",
    "            scores = list(new_dict[k]['emojis'].values())\n",
    "            if len(scores) == 0:\n",
    "                del new_dict[k]\n",
    "            else:\n",
    "                top_score = max(scores)\n",
    "                std_dev_score = np.std(scores)\n",
    "                for key in list(new_dict[k]['emojis'].keys()):\n",
    "                    if new_dict[k]['emojis'][key] < (top_score - std_dev_score):\n",
    "                        del new_dict[k]['emojis'][key]\n",
    "\n",
    "        with open(f'{filepath}.json', 'w') as f:\n",
    "            json.dump(new_dict, f)\n",
    "        \n",
    "        with open(f'{filepath}.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for word in new_dict:\n",
    "                writer.writerow([word, new_dict[word]['count'], new_dict[word]['emojis']])\n",
    "\n",
    "\n",
    "    def read_index(self, filepath: str):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.inverted_index = json.load(f)\n",
    "    \n",
    "    def read_meta(self, filepath: str):\n",
    "        self.emoji_dict = defaultdict()\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.emoji_dict = json.load(f)\n",
    "\n",
    "    def process_data(self, data):\n",
    "        with Pool(cpu_count()) as p:\n",
    "            p.map(self.index_data, data)\n",
    "\n",
    "i = Indexer(with_stopwords=False, group_emojis=False)\n",
    "i.indexer(\"good Good in with you a    luckðŸ˜®â€ðŸ’¨, good #you dawg qhoo . aah ðŸ‘ðŸ¼ðŸ˜®â€ðŸ’¨\")\n",
    "i.save_index(\"output/index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class QueryEngine:\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index.inverted_index\n",
    "        self.meta = index.emoji_dict\n",
    "        self.query_result = defaultdict(dict)\n",
    "\n",
    "    def process_query_score(self, search_query, cleaner, n_per_word=3, n_overall=5):\n",
    "        self.query_result = defaultdict(dict)\n",
    "        query = cleaner(search_query)\n",
    "        query_length = len(query)\n",
    "        for i, query_term in enumerate(query):\n",
    "            postings = self.index[query_term] if query_term in self.index else None\n",
    "            if postings is None:\n",
    "                continue\n",
    "           \n",
    "            for emo, score in postings['emojis'].items():\n",
    "                if emo not in self.query_result:\n",
    "                    self.query_result[emo] = {'query': query_term, 'raw': emo,'score': 0}\n",
    "                self.query_result[emo]['score'] += score / query_length\n",
    "\n",
    "        all_emojis = [(emoji, info) for emoji, info in self.query_result.items()]\n",
    "        all_emojis.sort(key=lambda x: x[1]['score'], reverse=True)\n",
    "        top_emojis = all_emojis[:n_overall]\n",
    "        return\", \".join(f\"{emoji.emojize(self.meta[emo[0]]['emoji'])}\" for emo in top_emojis) #  {emo[1]['score']:.2f}\n",
    "\n",
    "\n",
    "    def _positional_intersect(self, accumulator, newresults, k):\n",
    "        if accumulator is [] or newresults is None:\n",
    "            return accumulator\n",
    "        \n",
    "        answer = list()\n",
    "        for x_em, x_offsets in accumulator['emojis'].items():\n",
    "            for y_em, y_offsets in newresults['emojis'].items():\n",
    "                if(x_em == y_em):\n",
    "                    answer.append(x_em)\n",
    "        return answer\n",
    "    \n",
    "    def phrase_query(self, search_query, cleaner):\n",
    "        query = cleaner(search_query)\n",
    "        results = self.index[query[0]] if query[0] in self.index else None\n",
    "        for term in query[:1]:\n",
    "            matches = self.index[term] if term in self.index else None\n",
    "            results = self._positional_intersect(results, matches, 3)\n",
    "        print(results[:5])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518536\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "csv_files = glob.glob('data/clean/*.csv')\n",
    "\n",
    "for filename in csv_files:\n",
    "    preprocessor.load_data_from_csv(filename)\n",
    "preprocessor.combine_data()\n",
    "preprocessor.data.reset_index(drop=True)\n",
    "\n",
    "print(len(preprocessor.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task stop_group running!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a560dfc7b21e46f8ae40183d36572f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/518536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor done, writing to disk\n",
      "task stop_nogroup running!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9946afce03430da0cef558a8b39507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/518536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor done, writing to disk\n",
      "task nostop_group running!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165ee018013a494d8aa5784f76d14c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/518536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor done, writing to disk\n",
      "task nostop_nogroup running!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83dd2f17500422daf4da2ed44648aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/518536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor done, writing to disk\n"
     ]
    }
   ],
   "source": [
    "import swifter\n",
    "\n",
    "# Index data\n",
    "def index_data(with_stopwords: bool, group_emojis: bool, filename: str):\n",
    "    print(f'task {filename} running!')\n",
    "    indexer = Indexer(with_stopwords, group_emojis)\n",
    "    preprocessor.data['text'].swifter.apply(lambda x: indexer.indexer(x))\n",
    "    print(\"preprocessor done, writing to disk\")\n",
    "    indexer.save_index(f'output/{filename}')\n",
    "    indexer.save_metadata(f'output/{filename}_meta.json')\n",
    "\n",
    "\n",
    "index_data(True, True, \"stop_group\")\n",
    "index_data(True, False, \"stop_nogroup\")\n",
    "index_data(False, True, \"nostop_group\")\n",
    "index_data(False, False, \"nostop_nogroup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from file\n"
     ]
    }
   ],
   "source": [
    "print(\"reading data from file\")\n",
    "indexer_stop_group = Indexer(True, True)\n",
    "indexer_stop_group.read_index(\"output/stop_group.json\")\n",
    "indexer_stop_group.read_meta(\"output/stop_group_meta.json\")\n",
    "engine_stop_group = QueryEngine(indexer_stop_group)\n",
    "\n",
    "indexer_stop_nogroup = Indexer(True, False)\n",
    "indexer_stop_nogroup.read_index(\"output/stop_nogroup.json\")\n",
    "indexer_stop_nogroup.read_meta(\"output/stop_nogroup_meta.json\")\n",
    "engine_stop_nogroup = QueryEngine(indexer_stop_nogroup)\n",
    "\n",
    "indexer_nostop_group = Indexer(False, True)\n",
    "indexer_nostop_group.read_index(\"output/nostop_group.json\")\n",
    "indexer_nostop_group.read_meta(\"output/nostop_group_meta.json\")\n",
    "engine_nostop_group = QueryEngine(indexer_nostop_group)\n",
    "\n",
    "indexer_nostop_nogroup = Indexer(False, False)\n",
    "indexer_nostop_nogroup.read_index(\"output/nostop_nogroup.json\")\n",
    "indexer_nostop_nogroup.read_meta(\"output/nostop_nogroup_meta.json\")\n",
    "engine_nostop_nogroup = QueryEngine(indexer_nostop_nogroup)\n",
    "\n",
    "#engine.phrase_query(query, indexer._clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¯ðŸ’¯ðŸ’¯, ðŸš¬, âœŠðŸ¾âœŠðŸ¿, ðŸ’¯âœŠðŸ¾, ðŸ—\n",
      "ðŸ“, ðŸ¤œðŸ¼, ðŸ¤™ðŸ¿, ðŸ‘©ðŸ¼â€ðŸ³, ðŸ§“\n",
      "ðŸ™ŒðŸ», âœ‹ðŸ»ðŸ›‘, ðŸ¥³ðŸ‘ðŸ»ðŸš€ðŸ”¥ðŸ“ˆ, ðŸ’•ðŸ’•ðŸ’•ðŸ‘ðŸ¼âœ¨, ðŸ˜­ðŸ¤Œ\n",
      "ðŸšµâ€â™‚, ðŸšµâ€â™€, ðŸŸ¨, ðŸ—¨, ðŸŸ£\n"
     ]
    }
   ],
   "source": [
    "# Query data\n",
    "query = \"i need a bike now\"\n",
    "print(engine_stop_group.process_query_score(query, indexer_stop_group.tknizer.cleaner))\n",
    "print(engine_stop_nogroup.process_query_score(query, indexer_stop_nogroup.tknizer.cleaner))\n",
    "print(engine_nostop_group.process_query_score(query, indexer_nostop_group.tknizer.cleaner))\n",
    "print(engine_nostop_nogroup.process_query_score(query, indexer_nostop_nogroup.tknizer.cleaner))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I'm feeling happy.\",\n",
    "    \"I'm feeling very sad.\",\n",
    "    \"I'm angry with you.\",\n",
    "    \"I love pizza.\",\n",
    "    \"I dislike broccoli.\",\n",
    "    \"The sunrise this morning was beautiful.\",\n",
    "    \"It's been a long, tiring day.\",\n",
    "    \"I just won the lottery!\",\n",
    "    \"I can't believe we lost the game.\",\n",
    "    \"I'm so excited for the weekend.\",\n",
    "    \"The movie was boring.\",\n",
    "    \"That was the best concert ever!\",\n",
    "    \"I'm scared of spiders.\",\n",
    "    \"My heart is broken.\",\n",
    "    \"I can't wait for my birthday.\",\n",
    "    \"I am feeling so peaceful right now.\",\n",
    "    \"That joke was hilarious.\",\n",
    "    \"I'm feeling pretty indifferent about the whole situation.\",\n",
    "    \"I just got a promotion!\",\n",
    "    \"I feel like crying.\",\n",
    "    \"I can't stand the heat.\",\n",
    "    \"I am freezing!\",\n",
    "    \"That was a delicious meal.\",\n",
    "    \"I am on top of the world!\",\n",
    "    \"I just had a terrible day at work.\",\n",
    "    \"I'm worried about my exam.\",\n",
    "    \"That book was thrilling!\",\n",
    "    \"I'm feeling adventurous.\",\n",
    "    \"I'm feeling so lazy today.\",\n",
    "    \"That was a scary movie.\",\n",
    "    \"I am grateful for my friends.\",\n",
    "    \"The party was a blast!\",\n",
    "    \"That test was really hard.\",\n",
    "    \"I feel loved.\",\n",
    "    \"I feel so rejected.\",\n",
    "    \"I'm bursting with joy.\",\n",
    "    \"I'm disgusted by the trash.\",\n",
    "    \"That was a stressful situation.\",\n",
    "    \"I'm so proud of my team.\",\n",
    "    \"I'm amazed by the view.\",\n",
    "    \"That song was touching.\",\n",
    "    \"I feel so lonely.\",\n",
    "    \"I'm feeling nostalgic.\",\n",
    "    \"The race was intense.\",\n",
    "    \"That was an awkward conversation.\",\n",
    "    \"I feel inspired.\",\n",
    "    \"I'm feeling playful.\",\n",
    "    \"I'm feeling ambitious.\",\n",
    "    \"I'm feeling doubtful.\",\n",
    "    \"That was a surprising result.\",\n",
    "    \"I'm feeling content.\",\n",
    "    \"I'm so disappointed.\",\n",
    "    \"I'm feeling hopeful.\",\n",
    "    \"That was a frustrating experience.\",\n",
    "    \"I feel so appreciated.\",\n",
    "    \"I'm confused.\",\n",
    "    \"I'm feeling motivated.\",\n",
    "    \"I'm feeling pessimistic.\",\n",
    "    \"I'm feeling apathetic.\",\n",
    "    \"That was an impressive performance.\",\n",
    "    \"I'm curious about the result.\",\n",
    "    \"I'm feeling so relaxed.\",\n",
    "    \"I'm feeling agitated.\",\n",
    "    \"That was a depressing story.\",\n",
    "    \"I'm feeling optimistic.\",\n",
    "    \"I feel so empowered.\",\n",
    "    \"I'm feeling ashamed.\",\n",
    "    \"I'm feeling energized.\",\n",
    "    \"I'm feeling apprehensive.\",\n",
    "    \"I'm feeling delighted.\",\n",
    "    \"I'm feeling guilty.\",\n",
    "    \"That was a challenging puzzle.\",\n",
    "    \"I'm feeling so refreshed.\",\n",
    "    \"I'm feeling overwhelmed.\",\n",
    "    \"I'm feeling serene.\",\n",
    "    \"I'm feeling vulnerable.\",\n",
    "    \"That was a fascinating lecture.\",\n",
    "    \"I'm feeling proud.\",\n",
    "    \"I'm feeling humiliated.\",\n",
    "    \"I'm feeling so exhilarated.\",\n",
    "    \"I'm feeling regretful.\",\n",
    "    \"I'm feeling contented.\",\n",
    "    \"I'm feeling restless.\",\n",
    "    \"That was an enchanting evening.\",\n",
    "    \"I'm feeling tranquil.\",\n",
    "    \"I'm feeling tormented.\",\n",
    "    \"I'm feeling triumphant.\",\n",
    "    \"I'm feeling desolate.\",\n",
    "    \"I'm feeling blissful.\",\n",
    "    \"I'm feeling distressed.\",\n",
    "    \"I'm feeling jubilant.\",\n",
    "    \"I'm feeling woeful.\",\n",
    "    \"I'm feeling exuberant.\",\n",
    "    \"I'm feeling despondent.\",\n",
    "    \"I'm feeling ecstatic.\",\n",
    "    \"I'm feeling inconsolable.\",\n",
    "    \"I'm feeling rapturous.\",\n",
    "    \"I'm feeling forlorn.\",\n",
    "    \"I'm feeling exhilarated.\",\n",
    "    \"I'm feeling downhearted.\"\n",
    "]\n",
    "\n",
    "social_media_sentences = [\n",
    "    \"Just had the best coffee at @CafeLuv #CoffeeLover\",\n",
    "    \"Getting ready for a Friday night out with the girls! #FridayFeeling\",\n",
    "    \"Who else is excited for the new Avengers movie? #MarvelFan\",\n",
    "    \"Can't believe how beautiful the sunset was today. #NaturePhotography\",\n",
    "    \"Dinner at my favorite sushi place #Foodie\",\n",
    "    \"Throwback to my trip to Paris last summer #TravelDiaries\",\n",
    "    \"Feeling so blessed to have such amazing people in my life #Blessed\",\n",
    "    \"Workout done for the day! #FitnessGoals\",\n",
    "    \"I could spend all day reading at this quiet little bookstore #BookWorm\",\n",
    "    \"Had an awesome time at the concert last night! #LiveMusic\",\n",
    "    \"I can assist you with booking a flight #ChatBot\",\n",
    "    \"What can I help you find today? #CustomerService\",\n",
    "    \"Processing your request now... #AI\",\n",
    "    \"Your order has been placed! #ShoppingBot\",\n",
    "    \"The weather in New York today is sunny with a high of 75 degrees #WeatherBot\",\n",
    "    \"Directing you to a customer service representative now #HelpBot\",\n",
    "    \"That information is not currently available. Can I assist with anything else? #InfoBot\",\n",
    "    \"You have 3 new notifications #ReminderBot\",\n",
    "    \"You successfully completed your daily step goal! #HealthBot\",\n",
    "    \"Your package has been shipped and is on its way #DeliveryUpdate\"\n",
    "]\n",
    "\n",
    "bias_test_sentences = [\n",
    "    \"The strong woman lifted the heavy weights.\",\n",
    "    \"The Muslim man prayed at the mosque.\",\n",
    "    \"She is a successful businesswoman.\",\n",
    "    \"The African American artist created a beautiful painting.\",\n",
    "    \"The gay couple adopted a child.\",\n",
    "    \"He's a Jewish teacher in a high school.\",\n",
    "    \"The nurse worked a long night shift.\",\n",
    "    \"The Asian engineer solved the complex problem.\",\n",
    "    \"The transgender individual voiced their opinion.\",\n",
    "    \"The Latino chef cooked a delicious meal.\",\n",
    "    \"The homeless man found a job.\",\n",
    "    \"The Catholic nun helped the poor.\",\n",
    "    \"The elderly woman is a computer scientist.\",\n",
    "    \"The disabled athlete won the competition.\",\n",
    "    \"The construction worker is a female.\",\n",
    "    \"The Native American lawyer won the case.\",\n",
    "    \"The Hindu priest performed the rituals.\",\n",
    "    \"The single father took care of his children.\",\n",
    "    \"The young man is a ballet dancer.\",\n",
    "    \"The Indian woman is the CEO of the company.\"\n",
    "]\n",
    "\n",
    "name_test_sentences = [\n",
    "    \"Let's meet in New York City.\",\n",
    "    \"I love visiting Paris in the spring.\",\n",
    "    \"I just watched a movie starring Leonardo DiCaprio.\",\n",
    "    \"Did you hear the latest song by Beyonce?\",\n",
    "    \"John is a common name in the United States.\",\n",
    "    \"My best friend's name is Maria.\",\n",
    "    \"Mumbai is known for its delicious street food.\",\n",
    "    \"I just finished reading a book by J.K. Rowling.\",\n",
    "    \"I saw Tom Hanks at the airport today.\",\n",
    "    \"Emily is a popular name for girls.\",\n",
    "    \"I'm traveling to Sydney next month.\",\n",
    "    \"Jennifer Aniston is my favorite actress.\",\n",
    "    \"I attended a concert by the band Coldplay.\",\n",
    "    \"Beijing is a bustling city with a rich history.\",\n",
    "    \"I named my dog after Elon Musk.\",\n",
    "    \"Emma is a beautiful name for a baby girl.\",\n",
    "    \"I enjoyed the book written by Stephen King.\",\n",
    "    \"I'm planning a trip to Tokyo.\",\n",
    "    \"My favorite actor is Will Smith.\",\n",
    "    \"My name is Mohammed.\"\n",
    "]\n",
    "\n",
    "single_word_sentences = [\n",
    "    \"happy\",\n",
    "    \"angry\",\n",
    "    \"love\",\n",
    "    \"hate\",\n",
    "    \"food\",\n",
    "    \"hungry\",\n",
    "    \"tired\",\n",
    "    \"excited\",\n",
    "    \"work\",\n",
    "    \"home\",\n",
    "    \"play\",\n",
    "    \"game\",\n",
    "    \"sports\",\n",
    "    \"music\",\n",
    "    \"movie\",\n",
    "    \"book\",\n",
    "    \"travel\",\n",
    "    \"adventure\",\n",
    "    \"family\",\n",
    "    \"party\"\n",
    "]\n",
    "test_cases = [sentences, social_media_sentences, bias_test_sentences, name_test_sentences, single_word_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = list()\n",
    "\n",
    "for sentence in [sentence for case in test_cases for sentence in case]:\n",
    "    emojis_stop_group = engine_stop_group.process_query_score(sentence, indexer_stop_group.tknizer.cleaner)\n",
    "    emojis_stop_nogroup = engine_stop_nogroup.process_query_score(sentence, indexer_stop_nogroup.tknizer.cleaner)\n",
    "    emojis_nostop_group = engine_nostop_group.process_query_score(sentence, indexer_nostop_group.tknizer.cleaner)\n",
    "    emojis_nostop_nogroup = engine_nostop_nogroup.process_query_score(sentence, indexer_nostop_nogroup.tknizer.cleaner)\n",
    "    test_results.append({\"sentence\": sentence, \"stop_group\": emojis_stop_group, \"nostop_group\": emojis_nostop_group, \"stop_nogroup\": emojis_stop_nogroup, \"nostop_nogroup\": emojis_nostop_nogroup})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testresults_now.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for result in test_results:\n",
    "        #writer.writerow(['sg', result['sentence'], result['stop_group'],])\n",
    "        #writer.writerow(['s ', result['sentence'], result['stop_nogroup']])\n",
    "        writer.writerow([' g', result['sentence'], result['nostop_group']])\n",
    "        #writer.writerow(['--', result['sentence'], result['nostop_nogroup']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji: {'emoji': 'ðŸ‘', 'count': 10821}, Score: 9.34\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ¾', 'count': 10379}, Score: 9.38\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ¼', 'count': 10216}, Score: 9.4\n",
      "Emoji: {'emoji': 'ðŸ™ŒðŸ¾', 'count': 4189}, Score: 10.03\n",
      "Emoji: {'emoji': 'ðŸ‘ðŸ¼', 'count': 3479}, Score: 10.11\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ™ðŸ™', 'count': 2790}, Score: 10.18\n",
      "Emoji: {'emoji': 'ðŸ‘ðŸ¾', 'count': 2786}, Score: 10.18\n",
      "Emoji: {'emoji': 'ðŸ¤žðŸ¾', 'count': 2490}, Score: 10.22\n",
      "Emoji: {'emoji': 'ðŸ¤™', 'count': 1793}, Score: 10.3\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ¿', 'count': 1674}, Score: 10.31\n",
      "Emoji: {'emoji': 'ðŸ‘ŠðŸ½', 'count': 862}, Score: 10.41\n",
      "Emoji: {'emoji': 'ðŸ¤™ðŸ¾', 'count': 702}, Score: 10.43\n",
      "Emoji: {'emoji': 'ðŸ¤ŒðŸ¾', 'count': 714}, Score: 10.43\n",
      "Emoji: {'emoji': 'ðŸ’¯', 'count': 689}, Score: 10.43\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ¾ðŸ™ðŸ¾', 'count': 676}, Score: 10.43\n",
      "Emoji: {'emoji': 'â¤', 'count': 642}, Score: 10.44\n",
      "Emoji: {'emoji': 'ðŸ’ªðŸ¿', 'count': 563}, Score: 10.45\n",
      "Emoji: {'emoji': 'ðŸ™ŒðŸ¾ðŸ™ŒðŸ¾ðŸ™ŒðŸ¾', 'count': 493}, Score: 10.45\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ»â¤ï¸', 'count': 425}, Score: 10.46\n",
      "Emoji: {'emoji': 'ðŸ’ªðŸ½ðŸ’ªðŸ½', 'count': 419}, Score: 10.46\n",
      "Emoji: {'emoji': 'âœ¨', 'count': 485}, Score: 10.46\n",
      "Emoji: {'emoji': 'ðŸ‘‰ðŸ¼ðŸ‘ˆðŸ¼', 'count': 398}, Score: 10.47\n",
      "Emoji: {'emoji': 'ðŸ˜‰', 'count': 330}, Score: 10.47\n",
      "Emoji: {'emoji': 'ðŸ¤”', 'count': 386}, Score: 10.47\n",
      "Emoji: {'emoji': 'âœ…', 'count': 304}, Score: 10.48\n",
      "Emoji: {'emoji': 'ðŸ‘ŒðŸ‘Œ', 'count': 260}, Score: 10.48\n",
      "Emoji: {'emoji': 'ðŸ‘‰ðŸ½ðŸ‘ˆðŸ½', 'count': 295}, Score: 10.48\n",
      "Emoji: {'emoji': 'â¤ï¸ðŸ™ðŸ¼', 'count': 237}, Score: 10.49\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ¼ðŸ’¯', 'count': 93}, Score: 10.5\n",
      "Emoji: {'emoji': 'ðŸ˜­âœ‹', 'count': 112}, Score: 10.5\n",
      "Emoji: {'emoji': 'ðŸ™â™¥ï¸', 'count': 105}, Score: 10.5\n",
      "Emoji: {'emoji': 'ðŸ•', 'count': 38}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¶', 'count': 66}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸŒ­', 'count': 16}, Score: 10.51\n",
      "Emoji: {'emoji': 'âœŒï¸ðŸ˜”', 'count': 21}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ˜ðŸ˜ðŸ˜', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¤â‰ï¸', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ¼ðŸ”¥', 'count': 50}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ¼ðŸ’›', 'count': 21}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¶ðŸ‘¶ðŸ‘', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¶ðŸ˜Ž', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¤§ðŸ™', 'count': 4}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ‘ðŸ¾ðŸ˜‚', 'count': 3}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ¿ðŸ’›', 'count': 4}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¶ðŸ•', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¶ðŸ’¦ðŸ‘ðŸ¼ðŸ‘ðŸ¼â™¥ï¸', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'âœï¸', 'count': 26}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ–•ðŸ»ðŸ˜‚ðŸ¤£', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ’¯ðŸ’¯ðŸ’ªðŸ¾', 'count': 13}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¤žðŸ¾ðŸ’¯', 'count': 67}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ’¯ðŸ¤ŸðŸ½', 'count': 12}, Score: 10.51\n",
      "Emoji: {'emoji': 'âœ…ðŸ™ŒðŸ½ðŸŽ‰ðŸ¦ðŸªðŸ¥•ðŸ˜Ž', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ™ðŸ™â¤ï¸â¤ï¸â¤ï¸ðŸŒ»ðŸŒ»ðŸŒ»', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸŽ‰ðŸ¥³', 'count': 9}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ˜žðŸ‘', 'count': 3}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ’ªðŸ½ðŸ˜ˆ', 'count': 17}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ•ðŸ™Œ', 'count': 2}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ•ðŸŽ¶ðŸŽµ', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ’•ðŸ™', 'count': 45}, Score: 10.51\n",
      "Emoji: {'emoji': 'âœŠðŸ¾ðŸ’¯', 'count': 48}, Score: 10.51\n",
      "Emoji: {'emoji': 'âœŠðŸ¾ðŸ’ªðŸ¾', 'count': 11}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ‘ŽðŸ˜', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ’¯âœŠðŸ¾â¤ï¸', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ˜®\\u200dðŸ’¨âœ‹ðŸ»', 'count': 4}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ‘‡ðŸ¶', 'count': 3}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ˜ðŸ‘', 'count': 63}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸŒ±', 'count': 19}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ’“ðŸ¾ðŸ‘ðŸ½ðŸ‘ðŸ½', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ’ªðŸ½ðŸ’¯ðŸ™ðŸ½', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ–ðŸ¼', 'count': 50}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¤™ðŸ•ðŸ’¥', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¤ðŸ™…ðŸ»\\u200dâ™‚ï¸ðŸ', 'count': 1}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¤žðŸ™', 'count': 55}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ¦‹ðŸ™', 'count': 4}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ•', 'count': 3}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ™ðŸ¾ðŸˆ', 'count': 2}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ¦®', 'count': 14}, Score: 10.51\n",
      "Emoji: {'emoji': 'ðŸ•\\u200dðŸ¦º', 'count': 16}, Score: 10.51\n"
     ]
    }
   ],
   "source": [
    "ex = indexer_stop_group.inverted_index['dog']\n",
    "sorted_ex = sorted(ex['emojis'].items(), key=lambda item: item[1], reverse=False)\n",
    "for em, score in sorted_ex:\n",
    "        print(f\"Emoji: {indexer_stop_group.emoji_dict[em]}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji: {'emoji': 'ðŸ‘ðŸ¼', 'count': 1}, Score: [4]\n",
      "Emoji: {'emoji': 'ðŸ˜®\\u200dðŸ’¨', 'count': 2}, Score: [4, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "i = Indexer(False, False)\n",
    "i.read_index(\"output/index.json\")\n",
    "i.read_meta(\"output/index_meta.json\")\n",
    "#e = QueryEngine(i)\n",
    "\n",
    "ex = i.inverted_index['good']\n",
    "sorted_ex = sorted(ex['emojis'].items(), key=lambda item: item[1], reverse=False)\n",
    "for emoji, score in sorted_ex:\n",
    "        print(f\"Emoji: {i.emoji_dict[emoji]}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
