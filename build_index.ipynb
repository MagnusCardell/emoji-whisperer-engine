{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Unidecode nltk, emoji, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataPreprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def load_data_from_csv(self, filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        self.data.append(df)\n",
    "\n",
    "    def combine_data(self):\n",
    "        self.data = pd.concat(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "import emoji\n",
    "import string\n",
    "from itertools import tee\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "\n",
    "class Indexer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        self.emoji_dict = defaultdict()\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords += list(string.punctuation)\n",
    "        self.ps = PorterStemmer()\n",
    "        self.translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    def _clean_text(self, text: string):\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "        text = text.translate(self.translator)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [self.ps.stem(a) for a in words] #if a not in self.stopwords\n",
    "        return words\n",
    "\n",
    "    def index_data(self, text):\n",
    "        words = self._clean_text(text)\n",
    "        try:\n",
    "            emojis = []\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if(emoji.purely_emoji(word)):\n",
    "                    emojis.append(word)\n",
    "                    clean_words.append(word)\n",
    "                elif emoji.emoji_count(word) > 0:\n",
    "                    extracted_emojis = \"\".join([em[0] for em in emoji.analyze(word, join_emoji=True)])\n",
    "                    word = emoji.replace_emoji(word, replace=\"\")\n",
    "                    emojis.append(extracted_emojis)\n",
    "                    clean_words.append(word)\n",
    "                    clean_words.append(extracted_emojis)\n",
    "                else:\n",
    "                    clean_words.append(word)\n",
    "\n",
    "            current_words = []\n",
    "            for i, word in enumerate(clean_words):\n",
    "                if word not in self.inverted_index:\n",
    "                    self.inverted_index[word] = {'count': 0, 'emojis': {}}\n",
    "                if not emoji.purely_emoji(word):\n",
    "                    current_words.append(word)\n",
    "                else:    \n",
    "                    if word not in self.emoji_dict:\n",
    "                        self.emoji_dict[word] = 1\n",
    "                    else:\n",
    "                        self.emoji_dict[word] += 1\n",
    "                    self.inverted_index[word]['count'] += 1\n",
    "                    for e in emojis:\n",
    "                        emoji_offset = abs(clean_words.index(e) - i) + 1\n",
    "                        if(emoji_offset == 1): # itself\n",
    "                            continue\n",
    "                        if e not in self.inverted_index[word]['emojis']:\n",
    "                            self.inverted_index[word]['emojis'][e] = []\n",
    "                        self.inverted_index[word]['emojis'][e].append(emoji_offset)\n",
    "                    \n",
    "                    for w in current_words:\n",
    "                        self.inverted_index[w]['count'] += 1\n",
    "                        word_offset = abs(clean_words.index(w) - i) + 1\n",
    "                        if w not in self.inverted_index[w]['emojis']:\n",
    "                            self.inverted_index[w]['emojis'][word] = []\n",
    "                        self.inverted_index[w]['emojis'][word].append(word_offset)\n",
    "                    current_words = []\n",
    "        except StopIteration:\n",
    "            pass\n",
    "    \n",
    "    def save_metadata(self, filepath):\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.emoji_dict, f)\n",
    "\n",
    "    def save_index(self, filepath):\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.inverted_index, f)\n",
    "\n",
    "    def save_index_csv(self, filepath):\n",
    "        with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for word in self.inverted_index:\n",
    "                writer.writerow([word, self.inverted_index[word]['count'], self.inverted_index[word]['emojis']])\n",
    "\n",
    "    def read_index(self, filepath):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.inverted_index = json.load(f)\n",
    "    \n",
    "    def read_meta(self, filepath):\n",
    "        self.emoji_dict = defaultdict()\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.emoji_dict = json.load(f)\n",
    "\n",
    "# i = Indexer()\n",
    "# i.index_data(\"good good luck ğŸ˜ŠğŸ˜Š you dawgğŸ•ğŸ• qhooğŸ‘¨â€ğŸ‘©ğŸ¿â€ğŸ‘§ğŸ»â€ğŸ‘¦ğŸ¾ aah ğŸ˜®â€ğŸ’¨\")\n",
    "# i.save_index_csv(\"output/index.csv\")\n",
    "# i.save_index(\"output/index.json\")\n",
    "# i.save_metadata(\"output/meta.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class QueryEngine:\n",
    "\n",
    "    def __init__(self, index, meta):\n",
    "        self.index = index\n",
    "        self.meta = meta\n",
    "        self.query_result = defaultdict(dict)\n",
    "\n",
    "    # Vanilla (ish) tf-idf\n",
    "    def process_query_tf_idf(self, search_query, cleaner):\n",
    "        self.query_result = defaultdict(dict)\n",
    "        query = cleaner(search_query)\n",
    "        n = len(self.index)\n",
    "        for i, query_term in enumerate(query):\n",
    "            postings = self.index[query_term] if query_term in self.index else None\n",
    "            if postings is None:\n",
    "                continue\n",
    "            query_weight = 1.0\n",
    "            query_tf = len([q for q in query if q == query_term])\n",
    "            query_weight *= query_tf\n",
    "\n",
    "            #n = len(self.meta)\n",
    "            df_t = len(postings)\n",
    "            idf_t = math.log(n / df_t)\n",
    "            for emo, offset_list in postings['emojis'].items():\n",
    "                if emo not in self.query_result:\n",
    "                    self.query_result[emo] = {'query': search_query.split(' ')[i], 'raw': emo, 'emoji': emo, 'score': 0}\n",
    "                self.query_result[emo]['score'] += (query_tf * idf_t * postings['count']) \n",
    "            # normalize\n",
    "        for emo, info in self.query_result.items():\n",
    "            self.query_result[emo]['score'] /=  self.meta[emo]\n",
    "\n",
    "\n",
    "        return self.query_result\n",
    "    \n",
    "        # Vanilla (ish) tf-idf\n",
    "    def process_query_tf_idf2(self, search_query, cleaner):\n",
    "        self.query_result = defaultdict(dict)\n",
    "        query = cleaner(search_query)\n",
    "        n = len(self.index)\n",
    "        for i, query_term in enumerate(query):\n",
    "            postings = self.index[query_term] if query_term in self.index else None\n",
    "            if postings is None:\n",
    "                continue\n",
    "            query_weight = 1.0\n",
    "            query_tf = len([q for q in query if q == query_term])\n",
    "            query_weight *= query_tf\n",
    "\n",
    "            #n = len(self.meta)\n",
    "            df_t = len(postings)\n",
    "            idf_t = math.log(n / df_t)\n",
    "            for emo, offset_list in postings['emojis'].items():\n",
    "                avg_offset = sum(offset_list) / len(offset_list) if offset_list else 1\n",
    "                if emo not in self.query_result:\n",
    "                    self.query_result[emo] = {'query': search_query.split(' ')[i], 'raw': emo, 'emoji': emo, 'score': 0}\n",
    "                self.query_result[emo]['score'] += (query_tf * idf_t * postings['count']) / avg_offset\n",
    "            # normalize\n",
    "        for emo, info in self.query_result.items():\n",
    "            self.query_result[emo]['score'] /=  self.meta[emo]\n",
    "\n",
    "\n",
    "        return self.query_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_emojis(query_result, search, n_per_word=3, n_overall=5):\n",
    "    query_words = search.split(' ')\n",
    "        \n",
    "        # Print top emojis for each word\n",
    "    for word in query_words:\n",
    "        word_emojis = [(emoji, info['score']) for emoji, info in query_result.items() if info['query'] == word]\n",
    "        word_emojis.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_emojis = word_emojis[:n_per_word]\n",
    "        print(f\"For the word '{word}', the top {n_per_word} emojis are: {top_emojis}\")\n",
    "\n",
    "        # Print top emojis overall\n",
    "    all_emojis = [(emoji, info['score']) for emoji, info in query_result.items()]\n",
    "    all_emojis.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_emojis = all_emojis[:n_overall]\n",
    "    print(f\"The top {n_overall} emojis overall are: {top_emojis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "csv_files = glob.glob('data/clean/*.csv')\n",
    "\n",
    "for filename in csv_files:\n",
    "    preprocessor.load_data_from_csv(filename)\n",
    "preprocessor.combine_data()\n",
    "\n",
    "# Index data\n",
    "indexer = Indexer()\n",
    "\n",
    "preprocessor.data['text'].apply(lambda x: indexer.index_data(x))\n",
    "\n",
    "indexer.save_index('output/index.json')\n",
    "indexer.save_metadata('output/meta.json')\n",
    "indexer.save_index_csv('output/index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from file\n"
     ]
    }
   ],
   "source": [
    "print(\"reading data from file\")\n",
    "indexer = Indexer()\n",
    "indexer.read_index(\"output/index.json\")\n",
    "indexer.read_meta(\"output/meta.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the word 'gamer', the top 3 emojis are: [('ğŸ–¥ğŸ–±ğŸ˜ğŸ‘Š', 734.4372345011052), ('ğŸ˜ğŸ•¹', 734.4372345011052), ('ğŸ§ªğŸ®', 734.4372345011052)]\n",
      "The top 5 emojis overall are: [('ğŸ–¥ğŸ–±ğŸ˜ğŸ‘Š', 734.4372345011052), ('ğŸ˜ğŸ•¹', 734.4372345011052), ('ğŸ§ªğŸ®', 734.4372345011052), ('ğŸ†˜ğŸ†˜', 367.2186172505526), ('ğŸ®ğŸ®', 183.6093086252763)]\n"
     ]
    }
   ],
   "source": [
    "# Query data\n",
    "engine = QueryEngine(indexer.inverted_index, indexer.emoji_dict)\n",
    "query = \"gamer\"\n",
    "engine.process_query_tf_idf(query, indexer._clean_text)\n",
    "print_top_emojis(engine.query_result, query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.csv\", 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for word in self.inverted_index:\n",
    "                writer.writerow([word, self.inverted_index[word]['count'], self.inverted_index[word]['emojis']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
