{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Unidecode in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.3.6)\n",
      "Requirement already satisfied: nltk in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.8.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.0.2)\n",
      "Requirement already satisfied: autocorrect in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.6.1)\n",
      "Collecting swifter\n",
      "  Downloading swifter-1.3.5.tar.gz (490 kB)\n",
      "     ---------------------------------------- 0.0/490.6 kB ? eta -:--:--\n",
      "     ------- ------------------------------ 102.4/490.6 kB 3.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ----- 419.8/490.6 kB 5.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 430.1/490.6 kB 3.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 490.6/490.6 kB 3.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (1.24.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from swifter) (5.9.4)\n",
      "Collecting dask[dataframe]>=2.10.0\n",
      "  Downloading dask-2023.6.1-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     ---------------------------------------  1.2/1.2 MB 24.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 18.7 MB/s eta 0:00:00\n",
      "Collecting ipywidgets>=7.0.0\n",
      "  Downloading ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n",
      "     ---------------------------------------- 0.0/138.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 138.3/138.3 kB 8.0 MB/s eta 0:00:00\n",
      "Collecting cloudpickle>=0.2.2\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: parso>0.4.0 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from swifter) (0.8.3)\n",
      "Collecting bleach>=3.1.1\n",
      "  Downloading bleach-6.0.0-py3-none-any.whl (162 kB)\n",
      "     ---------------------------------------- 0.0/162.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 162.5/162.5 kB 10.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from bleach>=3.1.1->swifter) (1.16.0)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting partd>=1.2.0\n",
      "  Downloading partd-1.4.0-py3-none-any.whl (18 kB)\n",
      "Collecting fsspec>=2021.09.0\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "     ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 163.8/163.8 kB 10.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from dask[dataframe]>=2.10.0->swifter) (22.0)\n",
      "Collecting toolz>=0.10.0\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 0.0/55.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 55.8/55.8 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.3.1\n",
      "  Downloading PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 151.7/151.7 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata>=4.13.0\n",
      "  Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting jupyterlab-widgets~=3.0.7\n",
      "  Downloading jupyterlab_widgets-3.0.7-py3-none-any.whl (198 kB)\n",
      "     ---------------------------------------- 0.0/198.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 198.2/198.2 kB 6.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets>=7.0.0->swifter) (6.19.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets>=7.0.0->swifter) (8.7.0)\n",
      "Collecting widgetsnbextension~=4.0.7\n",
      "  Downloading widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     -------------------------------- ------- 1.7/2.1 MB 35.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 26.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets>=7.0.0->swifter) (5.8.0)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.2)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (24.0.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (7.4.8)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.1.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.5.6)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.6.4)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (3.0.36)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (2.13.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.6.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.18.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (5.1.1)\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (5.1.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from prompt-toolkit<3.1.0,>=3.0.11->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (1.2.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (2.6.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\carde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (305)\n",
      "Installing collected packages: webencodings, zipp, widgetsnbextension, toolz, pyyaml, locket, jupyterlab-widgets, fsspec, cloudpickle, bleach, partd, importlib-metadata, dask, ipywidgets, swifter\n",
      "  Running setup.py install for swifter: started\n",
      "  Running setup.py install for swifter: finished with status 'done'\n",
      "Successfully installed bleach-6.0.0 cloudpickle-2.2.1 dask-2023.6.1 fsspec-2023.6.0 importlib-metadata-6.7.0 ipywidgets-8.0.6 jupyterlab-widgets-3.0.7 locket-1.0.0 partd-1.4.0 pyyaml-6.0 swifter-1.3.5 toolz-0.12.0 webencodings-0.5.1 widgetsnbextension-4.0.7 zipp-3.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: swifter is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\carde\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install Unidecode nltk emoji pandas autocorrect swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataPreprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def load_data_from_csv(self, filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        self.data.append(df)\n",
    "\n",
    "    def combine_data(self):\n",
    "        self.data = pd.concat(self.data)\n",
    "        self.data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "import emoji\n",
    "import string\n",
    "from itertools import tee\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool, cpu_count\n",
    "class Indexer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        self.emoji_dict = defaultdict()\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords += list(string.punctuation)\n",
    "        self.ps = PorterStemmer()\n",
    "        self.tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=True)\n",
    "        #self.translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    def _clean_text(self, text: string):\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "        #text = text.translate(self.translator)\n",
    "        \n",
    "        words = self.tknzr.tokenize(text)\n",
    "        words = [self.ps.stem(a) for a in words] #if a not in self.stopwords\n",
    "        return words\n",
    "\n",
    "    def index_data(self, text):\n",
    "        words = self._clean_text(text)\n",
    "        try:\n",
    "            emojis = []\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if(emoji.purely_emoji(word)):\n",
    "                    #emojis.append(word)\n",
    "                    emojis.extend([em[0] for em in emoji.analyze(word, join_emoji=True)])\n",
    "                    #clean_words.append(word)\n",
    "                    clean_words.extend([em[0] for em in emoji.analyze(word, join_emoji=True)])\n",
    "                elif emoji.emoji_count(word) > 0:\n",
    "                    #extracted_emojis = \"\".join([em[0] for em in emoji.analyze(word, join_emoji=True)])\n",
    "                    extracted_emojis = [em[0] for em in emoji.analyze(word, join_emoji=True)]\n",
    "                    word = emoji.replace_emoji(word, replace=\"\")\n",
    "                    #emojis.append(extracted_emojis)\n",
    "                    emojis.extend(extracted_emojis)\n",
    "                    clean_words.append(word)\n",
    "                    #clean_words.append(extracted_emojis)\n",
    "                    clean_words.extend(extracted_emojis)\n",
    "                else:\n",
    "                    clean_words.append(word)\n",
    "\n",
    "            current_words = []\n",
    "            for i, word in enumerate(clean_words):\n",
    "                if word not in self.inverted_index:\n",
    "                    self.inverted_index[word] = {'count': 0, 'emojis': {}}\n",
    "                if not emoji.purely_emoji(word):\n",
    "                    current_words.append(word)\n",
    "                else:    \n",
    "                    if word not in self.emoji_dict:\n",
    "                        self.emoji_dict[word] = 1\n",
    "                    else:\n",
    "                        self.emoji_dict[word] += 1\n",
    "                    self.inverted_index[word]['count'] += 1\n",
    "                    for e in emojis:\n",
    "                        emoji_offset = abs(clean_words.index(e) - i) + 1\n",
    "                        if(emoji_offset == 1): # itself\n",
    "                            continue\n",
    "                        if e not in self.inverted_index[word]['emojis']:\n",
    "                            self.inverted_index[word]['emojis'][e] = []\n",
    "                        self.inverted_index[word]['emojis'][e].append(emoji_offset)\n",
    "                    \n",
    "                    for w in current_words:\n",
    "                        self.inverted_index[w]['count'] += 1\n",
    "                        word_offset = abs(clean_words.index(w) - i) + 1\n",
    "                        if w not in self.inverted_index[w]['emojis']:\n",
    "                            self.inverted_index[w]['emojis'][word] = []\n",
    "                        self.inverted_index[w]['emojis'][word].append(word_offset)\n",
    "                    current_words = []\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "    def save_metadata(self, filepath):\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.emoji_dict, f)\n",
    "\n",
    "    def save_index(self, filepath):\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.inverted_index, f)\n",
    "\n",
    "    def save_index_csv(self, filepath):\n",
    "        with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for word in self.inverted_index:\n",
    "                writer.writerow([word, self.inverted_index[word]['count'], self.inverted_index[word]['emojis']])\n",
    "\n",
    "    def read_index(self, filepath):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.inverted_index = json.load(f)\n",
    "    \n",
    "    def read_meta(self, filepath):\n",
    "        self.emoji_dict = defaultdict()\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.emoji_dict = json.load(f)\n",
    "\n",
    "    def process_data(self, data):\n",
    "        with Pool(cpu_count()) as p:\n",
    "            p.map(self.index_data, data)\n",
    "\n",
    "# i = Indexer()\n",
    "# i.index_data(\"good Good   lucküòÆ‚Äçüí®, you dawg qhoo . aah üòÆ‚Äçüí®\")\n",
    "# i.save_index_csv(\"output/index.csv\")\n",
    "# i.save_index(\"output/index.json\")\n",
    "# i.save_metadata(\"output/meta.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import takewhile\n",
    "class QueryEngine:\n",
    "\n",
    "    def __init__(self, index, meta):\n",
    "        self.index = index\n",
    "        self.meta = meta\n",
    "        self.query_result = defaultdict(dict)\n",
    "\n",
    "    def _findMedian(self, a):\n",
    "        # First we sort the array\n",
    "        sorted(a)\n",
    "    \n",
    "        # check for even case\n",
    "        n = len(a)\n",
    "        if n % 2 != 0:\n",
    "            return float(a[int(n/2)])\n",
    "    \n",
    "        return float((a[int((n-1)/2)] +\n",
    "                    a[int(n/2)])/2.0)\n",
    "    # Vanilla (ish) tf-idf\n",
    "    def process_query_tf_idf(self, search_query, cleaner, n_per_word=3, n_overall=5):\n",
    "        self.query_result = defaultdict(dict)\n",
    "        query = cleaner(search_query)\n",
    "        print(query)\n",
    "        n = len(self.index)\n",
    "        for i, query_term in enumerate(query):\n",
    "            postings = self.index[query_term] if query_term in self.index else None\n",
    "            if postings is None:\n",
    "                continue\n",
    "            query_weight = 1.0\n",
    "            query_tf = len([q for q in query if q == query_term])\n",
    "            query_weight *= query_tf\n",
    "\n",
    "            #n = len(self.meta)\n",
    "            df_t = len(postings)\n",
    "            idf_t = math.log(n / df_t)\n",
    "            for emo, offset_list in postings['emojis'].items():\n",
    "                median = self._findMedian(offset_list)\n",
    "                if emo not in self.query_result:\n",
    "                    self.query_result[emo] = {'query': search_query.split(' ')[i], 'raw': emo, 'emoji': emo, 'score': 0}\n",
    "                self.query_result[emo]['score'] += (query_tf * idf_t) / median\n",
    "                #print(emo, self.query_result[emo]['score'])\n",
    "            # normalize\n",
    "        # for emo, info in self.query_result.items():\n",
    "        #     self.query_result[emo]['score'] /=  self.meta[emo]\n",
    "        all_emojis = [(emoji, info['score']) for emoji, info in self.query_result.items()]\n",
    "        all_emojis.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_emojis = all_emojis[:n_overall]\n",
    "        print(f\"The top {n_overall} emojis overall are:\")\n",
    "        for x in top_emojis:\n",
    "            print(x)\n",
    "\n",
    "    \n",
    "        # Vanilla (ish) tf-idf\n",
    "    def process_query_tf_idf2(self, search_query, cleaner):\n",
    "        self.query_result = defaultdict(dict)\n",
    "        query = cleaner(search_query)\n",
    "        n = len(self.index)\n",
    "        for i, query_term in enumerate(query):\n",
    "            postings = self.index[query_term] if query_term in self.index else None\n",
    "            if postings is None:\n",
    "                continue\n",
    "            query_weight = 1.0\n",
    "            query_tf = len([q for q in query if q == query_term])\n",
    "            query_weight *= query_tf\n",
    "\n",
    "            #n = len(self.meta)\n",
    "            df_t = len(postings)\n",
    "            idf_t = math.log(n / df_t)\n",
    "            for emo, offset_list in postings['emojis'].items():\n",
    "                avg_offset = sum(offset_list) / len(offset_list) if offset_list else 1\n",
    "                if emo not in self.query_result:\n",
    "                    self.query_result[emo] = {'query': search_query.split(' ')[i], 'raw': emo, 'emoji': emo, 'score': 0}\n",
    "                self.query_result[emo]['score'] += (query_tf * idf_t ) / avg_offset\n",
    "            # normalize\n",
    "        # for emo, info in self.query_result.items():\n",
    "        #     self.query_result[emo]['score'] /=  self.meta[emo]\n",
    "        return self.query_result\n",
    "\n",
    "\n",
    "    def _positional_intersect(self, accumulator, newresults, k):\n",
    "        if accumulator is [] or newresults is None:\n",
    "            return accumulator\n",
    "        \n",
    "        answer = list()\n",
    "        for x_em, x_offsets in accumulator['emojis'].items():\n",
    "            for y_em, y_offsets in newresults['emojis'].items():\n",
    "                if(x_em == y_em):\n",
    "                    answer.append(x_em)\n",
    "        return answer\n",
    "    \n",
    "    def phrase_query(self, search_query, cleaner):\n",
    "        query = cleaner(search_query)\n",
    "        print(query)\n",
    "        results = self.index[query[0]] if query[0] in self.index else None\n",
    "        for term in query[:1]:\n",
    "            matches = self.index[term] if term in self.index else None\n",
    "            results = self._positional_intersect(results, matches, 3)\n",
    "        print(results[:5])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_emojis(query_result, search, n_per_word=3, n_overall=5):\n",
    "    query_words = search.split(' ')\n",
    "        \n",
    "        # Print top emojis for each word\n",
    "    for word in query_words:\n",
    "        word_emojis = [(emoji, info['score']) for emoji, info in query_result.items() if info['query'] == word]\n",
    "        word_emojis.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_emojis = word_emojis[:n_per_word]\n",
    "        print(f\"For the word '{word}', the top {n_per_word} emojis are: {top_emojis}\")\n",
    "\n",
    "        # Print top emojis overall\n",
    "    all_emojis = [(emoji, info['score']) for emoji, info in query_result.items()]\n",
    "    all_emojis.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_emojis = all_emojis[:n_overall]\n",
    "    print(f\"The top {n_overall} emojis overall are:\")\n",
    "    for x in top_emojis:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403feac11bb64fa0b097e6cf81d2f1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1516115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor done, writing to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "import swifter\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "csv_files = glob.glob('data/clean/*.csv')\n",
    "\n",
    "for filename in csv_files:\n",
    "    preprocessor.load_data_from_csv(filename)\n",
    "preprocessor.combine_data()\n",
    "preprocessor.data.reset_index(drop=True)\n",
    "# Index data\n",
    "indexer = Indexer()\n",
    "\n",
    "preprocessor.data['text'].swifter.apply(lambda x: indexer.index_data(x))\n",
    "print(\"preprocessor done, writing to disk\")\n",
    "\n",
    "indexer.save_index('output/index.json')\n",
    "indexer.save_metadata('output/meta.json')\n",
    "indexer.save_index_csv('output/index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from file\n"
     ]
    }
   ],
   "source": [
    "print(\"reading data from file\")\n",
    "indexer = Indexer()\n",
    "indexer.read_index(\"output/index.json\")\n",
    "indexer.read_meta(\"output/meta.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'food', 'wa', 'super']\n",
      "['üë®\\u200düë©\\u200düëß\\u200düë¶', 'üõ©', '3Ô∏è‚É£', 'üö´', 'üí∞']\n",
      "['the', 'food', 'wa', 'super']\n",
      "The top 5 emojis overall are:\n",
      "('üòØ', 16.035256155404856)\n",
      "('ü§©', 15.506621337094806)\n",
      "('üëèüèæ', 14.92512303695375)\n",
      "('üë•', 14.537457503526381)\n",
      "('üòµ', 14.537457503526381)\n"
     ]
    }
   ],
   "source": [
    "# Query data\n",
    "engine = QueryEngine(indexer.inverted_index, indexer.emoji_dict)\n",
    "query = \"the food was super\"\n",
    "engine.phrase_query(query, indexer._clean_text)\n",
    "engine.process_query_tf_idf(query, indexer._clean_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.csv\", 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for word in self.inverted_index:\n",
    "                writer.writerow([word, self.inverted_index[word]['count'], self.inverted_index[word]['emojis']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
