{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Unidecode nltk, emoji, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataPreprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def load_data_from_csv(self, filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        self.data.append(df)\n",
    "\n",
    "    def combine_data(self):\n",
    "        self.data = pd.concat(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "import emoji\n",
    "import string\n",
    "from itertools import tee\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "\n",
    "class Indexer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords += list(string.punctuation)\n",
    "        self.ps = PorterStemmer()\n",
    "        self.translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    def _clean_text(self, text: string):\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "        text = text.translate(self.translator)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        #words = [self.ps.stem(a) for a in words if a not in self.stopwords]\n",
    "        return words\n",
    "\n",
    "    def index_data(self, text):\n",
    "        words = self._clean_text(text)\n",
    "        try:\n",
    "            emojis = []\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if(emoji.purely_emoji(word)):\n",
    "                    emojis.append(word)\n",
    "                    clean_words.append(word)\n",
    "                elif emoji.emoji_count(word) > 0:\n",
    "                    extracted_emojis = \"\".join([em['emoji'] for em in emoji.emoji_list(word)])\n",
    "                    word = emoji.replace_emoji(word, replace=\"\")\n",
    "                    emojis.append(extracted_emojis)\n",
    "                    clean_words.append(word)\n",
    "                    clean_words.append(extracted_emojis)\n",
    "                else:\n",
    "                    clean_words.append(word)\n",
    "\n",
    "            current_words = []\n",
    "            for i, word in enumerate(clean_words):\n",
    "                if word not in self.inverted_index:\n",
    "                    self.inverted_index[word] = {'count': 0, 'emojis': {}}\n",
    "                if not emoji.purely_emoji(word):\n",
    "                    current_words.append(word)\n",
    "                else:    \n",
    "                    self.inverted_index[word]['count'] += 1\n",
    "                    for e in emojis:\n",
    "                        emoji_offset = abs(clean_words.index(e) - i) + 1\n",
    "                        if(emoji_offset == 1): # itself\n",
    "                            continue\n",
    "                        if e not in self.inverted_index[word]['emojis']:\n",
    "                            self.inverted_index[word]['emojis'][e] = []\n",
    "                        self.inverted_index[word]['emojis'][e].append(emoji_offset)\n",
    "                    \n",
    "                    for w in current_words:\n",
    "                        self.inverted_index[w]['count'] += 1\n",
    "                        word_offset = abs(clean_words.index(w) - i) + 1\n",
    "                        if w not in self.inverted_index[w]['emojis']:\n",
    "                            self.inverted_index[w]['emojis'][word] = []\n",
    "                        self.inverted_index[w]['emojis'][word].append(word_offset)\n",
    "                    current_words = []\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "    def save_index(self, filepath):\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.inverted_index, f)\n",
    "\n",
    "    def save_index_csv(self, filepath):\n",
    "        with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for word in self.inverted_index:\n",
    "                writer.writerow([word, self.inverted_index[word]['count'], self.inverted_index[word]['emojis']])\n",
    "\n",
    "    def read_index(self, filepath):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.inverted_index = json.load(f)\n",
    "    \n",
    "# i = Indexer()\n",
    "# i.index_data(\"good good luck üòäüòä you dawgüêïüêï\")\n",
    "# i.save_index_csv(\"output/index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class QueryEngine:\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "        self.query_result = defaultdict(dict)\n",
    "\n",
    "    def process_query_avg_offset(self, search_query, index):\n",
    "        self.query_result = defaultdict(dict)\n",
    "        query = index._clean_text(search_query)\n",
    "        query_weight = 1/len(query)\n",
    "        print(query, query_weight)\n",
    "        for i, q in enumerate(query):\n",
    "            matching_emojis = self.index[q]\n",
    "\n",
    "            query_tf = query_weight\n",
    "            n = len(self.index)\n",
    "            df_t = len(matching_emojis['emojis']) if matching_emojis else -1\n",
    "            if df_t == -1:\n",
    "                continue\n",
    "            idf_t = math.log(n / df_t)\n",
    "\n",
    "            emoji_counter = {}\n",
    "            for emo, counts in matching_emojis['emojis'].items():\n",
    "                if emo not in emoji_counter:\n",
    "                    emoji_counter[emo] = {'count':1, 'offsets':counts}\n",
    "                else:\n",
    "                    emoji_counter[emo]['count'] += 1\n",
    "\n",
    "            for unique_emoji, info in emoji_counter.items():\n",
    "                avg_offset = sum(info['offsets']) / len(info['offsets']) if info['offsets'] else 1\n",
    "                if unique_emoji not in self.query_result:\n",
    "                    self.query_result[unique_emoji] = {'query': search_query.split(' ')[i], 'raw': q, 'emoji': unique_emoji, 'score': (query_tf * idf_t * info['count']) / avg_offset}\n",
    "                else:\n",
    "                    self.query_result[unique_emoji]['score'] += (query_tf * idf_t *  info['count']) / avg_offset\n",
    "        #normalize tf_idf on length\n",
    "        for emoji, info in self.query_result.items():\n",
    "            info['score'] /= self.index[info['raw']]['count']\n",
    "            self.query_result[emoji] = info\n",
    "        return self.query_result\n",
    "\n",
    "    def process_query(self, search_query, index):\n",
    "        self.query_result = defaultdict(dict)\n",
    "        query = index._clean_text(search_query)\n",
    "        n = len(self.index)\n",
    "        d = 0.5\n",
    "\n",
    "        for i, q in enumerate(query):\n",
    "            matching_emojis = self.index[q] if q in self.index else None\n",
    "\n",
    "            if not matching_emojis:\n",
    "                continue\n",
    "\n",
    "            # vanilla tf-idf\n",
    "            term_freq = matching_emojis['count']\n",
    "            \n",
    "            # sublinear tf scaling\n",
    "            #term_freq = 1 + math.log(term_freq)\n",
    "\n",
    "            # max tf normalization\n",
    "            #term_freq = 0.5 + 0.5 * (term_freq / max_tf)\n",
    "\n",
    "            # custome offset based tf scaling\n",
    "            df = len(matching_emojis['emojis'])\n",
    "            idf = math.log(n / df)\n",
    "\n",
    "            for emo, offset_list in matching_emojis['emojis'].items():\n",
    "                #unique_emoji = individual_emoji['emoji']\n",
    "                avg_offset = sum(offset_list) / len(offset_list) \n",
    "                tf = 1 + math.log(sum(math.pow(d, p) for p in offset_list))\n",
    "\n",
    "\n",
    "                if emo not in self.query_result:\n",
    "                    self.query_result[emo] = {'query': search_query.split(' ')[i], 'raw': q, 'emoji': emo, 'score': 0}\n",
    "\n",
    "                self.query_result[emo]['score'] += (tf * idf *  info['count']) / avg_offset #term_freq * idf \n",
    "\n",
    "        # normalize scores\n",
    "        for emoji, info in self.query_result.items():\n",
    "            self.query_result[emoji]['score'] /=  self.index[info['raw']]['count']\n",
    "\n",
    "        return self.query_result\n",
    "    \n",
    "    def print_top_emojis(self, search, n_per_word=3, n_overall=5):\n",
    "        query_words = search.split(' ')\n",
    "        \n",
    "        # Print top emojis for each word\n",
    "        for word in query_words:\n",
    "            word_emojis = [(emoji, info['score']) for emoji, info in self.query_result.items() if info['query'] == word]\n",
    "            word_emojis.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_emojis = word_emojis[:n_per_word]\n",
    "            print(f\"For the word '{word}', the top {n_per_word} emojis are: {top_emojis}\")\n",
    "\n",
    "        # Print top emojis overall\n",
    "        all_emojis = [(emoji, info['score']) for emoji, info in self.query_result.items()]\n",
    "        all_emojis.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_emojis = all_emojis[:n_overall]\n",
    "        print(f\"The top {n_overall} emojis overall are: {top_emojis}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "csv_files = glob.glob('data/clean/*.csv')\n",
    "\n",
    "for filename in csv_files:\n",
    "    preprocessor.load_data_from_csv(filename)\n",
    "preprocessor.combine_data()\n",
    "\n",
    "# Index data\n",
    "indexer = Indexer()\n",
    "\n",
    "preprocessor.data['text'].apply(lambda x: indexer.index_data(x))\n",
    "\n",
    "indexer.save_index('output/index.json')\n",
    "indexer.save_index_csv('output/index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading data from file\")\n",
    "indexer = Indexer()\n",
    "indexer.read_index(\"output/index.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'weather', 'will', 'be', 'sunny', 'tomorrow'] 0.16666666666666666\n",
      "For the word 'the', the top 3 emojis are: [('üòú', 1.2974020779344543e-05), ('üì∏', 1.2676716175715308e-05), ('üíù', 1.1401735218412606e-05)]\n",
      "For the word 'weather', the top 3 emojis are: []\n",
      "For the word 'will', the top 3 emojis are: [('‚ùî', 4.114805922383699e-05), ('üè§', 3.239335048636906e-05), ('üöã', 2.0621181162178596e-05)]\n",
      "For the word 'be', the top 3 emojis are: [('üÖ±', 9.753242092682357e-06), ('‚ôí', 4.8766210463411784e-06), ('üõ¨', 3.901296837072943e-06)]\n",
      "For the word 'sunny', the top 3 emojis are: [('‚òÄÔ∏è', 0.0036245079061921356)]\n",
      "For the word 'tomorrow', the top 3 emojis are: [('üïí', 6.725831613487132e-05), ('üïê', 6.725831613487132e-05), ('üï¢', 6.725831613487132e-05)]\n",
      "The top 5 emojis overall are: [('‚òÄÔ∏è', 0.0036245079061921356), ('üïí', 6.725831613487132e-05), ('üïê', 6.725831613487132e-05), ('üï¢', 6.725831613487132e-05), ('‚õ©', 4.483887742324754e-05)]\n"
     ]
    }
   ],
   "source": [
    "# Query data\n",
    "    \n",
    "engine = QueryEngine(indexer.inverted_index)\n",
    "\n",
    "query = \"the weather will be sunny tomorrow\"\n",
    "engine.process_query_avg_offset(query, indexer)\n",
    "engine.print_top_emojis(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "import emoji\n",
    "import string\n",
    "from itertools import tee\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "\n",
    "class Indexer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords += list(string.punctuation)\n",
    "        self.ps = PorterStemmer()\n",
    "        self.translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    def _clean_text(self, text: string):\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "        text = text.translate(self.translator)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        #words = [self.ps.stem(a) for a in words if a not in self.stopwords]\n",
    "        return words\n",
    "\n",
    "    def index_data(self, text):\n",
    "        words = self._clean_text(text)\n",
    "        try:\n",
    "            emojis = []\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if(emoji.purely_emoji(word)):\n",
    "                    emojis.append(word)\n",
    "                    clean_words.append(word)\n",
    "                elif emoji.emoji_count(word) > 0:\n",
    "                    extracted_emojis = \"\".join([em['emoji'] for em in emoji.emoji_list(word)])\n",
    "                    print(extracted_emojis)\n",
    "                    word = emoji.replace_emoji(word, replace=\"\")\n",
    "                    emojis.append(extracted_emojis)\n",
    "                    clean_words.append(word)\n",
    "                    clean_words.append(extracted_emojis)\n",
    "                else:\n",
    "                    clean_words.append(word)\n",
    "\n",
    "            for i, word in enumerate(clean_words):\n",
    "                if word not in self.inverted_index:\n",
    "                    self.inverted_index[word] = {'count': 0, 'emojis': {}}\n",
    "                self.inverted_index[word]['count'] += 1\n",
    "                for em in emojis:\n",
    "                    emoji_offset = abs(clean_words.index(em) - i) + 1\n",
    "                    if em not in self.inverted_index[word]['emojis']:\n",
    "                        self.inverted_index[word]['emojis'][em] = []\n",
    "                    self.inverted_index[word]['emojis'][em].append(emoji_offset)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "    def save_index(self, filepath):\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.inverted_index, f)\n",
    "\n",
    "    def save_index_csv(self, filepath):\n",
    "        with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for word in self.inverted_index:\n",
    "                writer.writerow([word, self.inverted_index[word]['count'], self.inverted_index[word]['emojis']])\n",
    "\n",
    "    def read_index(self, filepath):\n",
    "        self.inverted_index = defaultdict(dict)\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.inverted_index = json.load(f)\n",
    "    \n",
    "i = Indexer()\n",
    "i.index_data(\"good good luck üòäüòä you dawgüêïüêï\")\n",
    "i.save_index_csv(\"output/index.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
